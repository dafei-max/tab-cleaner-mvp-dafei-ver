# 搜索系统原理说明

## 整体架构

```
用户输入查询 → 查询向量化 → 相似度计算 → 排序 → 返回结果
                ↓
        文档向量化（预处理阶段）
```

## 详细流程

### 阶段1：文档预处理与向量化（`process_opengraph_for_search`）

#### 1.1 文本处理
- **输入**：OpenGraph 数据（title + description）
- **处理**：提取文本内容（`extract_text_from_item`）
- **输出**：文本字符串（最多 32000 字符）

#### 1.2 图像处理（`preprocess.py`）
- **下载**：从 URL 下载图像（`download_image`）
- **预处理**：
  - 小图放大：如果 < 100px，放大到至少 100px
  - 大图缩小：如果 > 1024px，等比例缩小到 1024px
  - 格式转换：RGBA/LA/P → RGB
  - 压缩：JPEG 质量 85%，最大 20MB
- **编码**：转换为 Base64 字符串

#### 1.3 文本向量化（`embed_text.py`）
- **API**：DashScope `text-embedding-v4`
- **输入**：文本字符串
- **输出**：1024 维向量
- **特点**：语义理解，支持模糊匹配

#### 1.4 图像向量化（`embed_image.py`）
- **API**：DashScope `tongyi-embedding-vision-plus`
- **输入**：Base64 图像数据
- **输出**：1152 维向量
- **特点**：视觉理解，支持图像语义搜索

#### 1.5 向量融合（`fuse.py`）
- **方法**：加权融合文本向量和图像向量
- **权重选择**（根据内容类型自适应）：
  - 默认：文本 60% + 图像 40%
  - 图片为主（Pinterest/小红书等）：文本 10% + 图像 90%
  - 文档为主（GitHub/docs等）：文本 80% + 图像 20%
- **维度处理**：
  - 如果维度匹配：直接归一化后加权融合
  - 如果维度不匹配：使用 PCA 降维（保留主要信息），而不是简单截断
- **输出**：融合后的向量（同时保存 `text_embedding` 和 `image_embedding` 用于后续分别计算）

### 阶段2：搜索查询（`search_relevant_items`）

#### 2.1 查询向量化
- **输入**：用户输入的查询文本（如 "chair"）
- **处理**：使用 `text-embedding-v4` 生成查询向量（1024 维）
- **输出**：查询向量

#### 2.2 相似度计算（`rank.py`）

**核心创新：分别计算相似度，然后融合分数**

##### 方法1：分别计算相似度（推荐，`use_separate_similarity=True`）

```
对于每个文档：
1. 文本相似度 = cosine_similarity(查询向量[1024维], 文档文本向量[1024维])
2. 图像相似度 = cosine_similarity(查询向量[1024维], 文档图像向量[1152维，使用PCA降维到1024维])
3. 最终相似度 = 0.6 × 文本相似度 + 0.4 × 图像相似度
```

**优势**：
- ✅ 避免维度不匹配问题
- ✅ 保留完整文本信息（文本相似度不受影响）
- ✅ 图像信息损失最小（PCA降维保留主要信息）
- ✅ 灵活调整权重（根据内容类型）

##### 方法2：使用融合向量（备选）

```
最终相似度 = cosine_similarity(查询向量[1024维], 融合向量[1024维])
```

**缺点**：
- ❌ 图像信息可能被截断丢失
- ❌ 维度不匹配时需要处理

#### 2.3 余弦相似度计算（`fuse.py`）

公式：
```
cos(θ) = (A · B) / (||A|| × ||B||)
```

特点：
- 值范围：[-1, 1]，通常为 [0, 1]
- 值越大，越相似
- 归一化向量：只关注方向，不关注大小

#### 2.4 排序与返回

1. 按相似度从高到低排序
2. 返回 top_k 个结果（默认 20 个）
3. 每个结果包含：
   - 原始 OpenGraph 数据
   - 相似度分数（0-1）
   - Embedding 向量（可选）

### 阶段3：兜底机制

如果 API 调用失败或没有 embedding：
- **本地模糊搜索**（`fuzzy_score`）：
  - 关键词匹配（title + description）
  - 词频统计
  - 标题匹配加权

## 数据流示例

```
原始数据：
{
  "title": "设计椅子",
  "description": "现代简约风格",
  "image": "https://example.com/chair.jpg"
}

↓ 预处理

文本：["设计椅子", "现代简约风格"] → "设计椅子 现代简约风格"
图像：下载 → 缩放 → Base64编码

↓ Embedding

文本向量：[0.1, 0.2, ..., 0.9] (1024维)
图像向量：[0.3, 0.4, ..., 0.8] (1152维)

↓ 融合（可选，用于存储）

融合向量：[0.18, 0.28, ..., 0.85] (1024维，加权融合)

↓ 搜索（用户输入："chair"）

查询向量：[0.15, 0.25, ..., 0.9] (1024维)

↓ 相似度计算

文本相似度 = cosine(查询向量, 文本向量) = 0.85
图像相似度 = cosine(查询向量, PCA(图像向量)) = 0.72
最终相似度 = 0.6 × 0.85 + 0.4 × 0.72 = 0.798

↓ 排序

返回 top_k 个最相似的结果
```

## 关键技术点

### 1. 维度不匹配处理
- **问题**：文本向量 1024 维，图像向量 1152 维
- **解决方案**：
  - 分别计算相似度（推荐）
  - PCA 降维（保留主要信息）
  - 不简单截断（避免信息损失）

### 2. 自适应权重
- 根据内容类型自动调整文本/图像权重
- 图片为主：图像权重 90%
- 文档为主：文本权重 80%
- 默认：文本 60% + 图像 40%

### 3. 错误处理
- 图像下载失败：只使用文本向量
- Embedding 失败：使用本地模糊搜索
- 部分失败不影响整体

### 4. 性能优化
- 批量处理：每次处理多个文档
- 节流控制：API 调用间隔（150ms）
- 图像预处理：压缩和缩放减少 token 消耗

## 为什么准确率高？

1. **语义理解**：使用 AI Embedding，不是关键词匹配
2. **多模态融合**：同时考虑文本和图像
3. **分别计算相似度**：避免维度不匹配导致的信息损失
4. **自适应权重**：根据内容类型调整权重
5. **PCA降维**：保留主要信息，而非简单截断

## 与简单截断的对比

| 方法 | 文本信息 | 图像信息 | 准确率 |
|------|---------|---------|--------|
| 简单截断 | ✅ 完整 | ❌ 丢失128维 | ⭐⭐ |
| 分别计算相似度 | ✅ 完整 | ✅ PCA降维保留主要信息 | ⭐⭐⭐⭐⭐ |

## 总结

整个系统采用**多模态语义搜索**，通过：
1. 文本和图像分别向量化
2. 自适应权重融合
3. 分别计算相似度后融合分数
4. PCA降维而非简单截断

实现了**高准确率**的模糊搜索，支持语义理解，不依赖精确关键词匹配。




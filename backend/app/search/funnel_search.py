"""
ä¸‰é˜¶æ®µæ¼æ–—æœç´¢æ¨¡å—
1. ç²—å¬å›ï¼ˆMulti-Recallï¼‰ï¼šå¤šè·¯å¾„å¬å› 100-200 ä¸ªå€™é€‰
2. ç²¾æ’åºï¼ˆRe-Rankingï¼‰ï¼šèåˆ 5 è·¯åˆ†æ•°
3. åŠ¨æ€è¿‡æ»¤ï¼ˆThreshold Filteringï¼‰ï¼šæ ¹æ®è´¨é‡åŠ¨æ€è¿”å› 1-20 ä¸ªç»“æœ
"""
from typing import List, Dict, Optional, Tuple
import asyncio
from .threshold_filter import FilterMode, filter_by_threshold
from .smart_filter import smart_filter
from .embed import embed_text, embed_image
from .fuse import cosine_similarity, fuse_similarity_scores
from .rank import fuzzy_score
from .query_enhance import enhance_visual_query
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

from vector_db import (
    search_by_text_embedding,
    search_by_image_embedding,
    search_by_caption_embedding,
    get_pool,
    ACTIVE_TABLE,
    NAMESPACE,
    _normalize_user_id,
    to_vector_str,
    _row_to_dict,
)
import asyncpg


# äº”è·¯åˆ†æ•°æƒé‡é…ç½®ï¼ˆä» fusion_weights æ¨¡å—å¯¼å…¥ï¼Œå¯é…ç½®ï¼‰
from .fusion_weights import FUSION_WEIGHTS
from .config import MIN_SIMILARITY_THRESHOLD, IMAGE_EMBEDDING_THRESHOLD, CAPTION_RANK_THRESHOLD


async def _coarse_recall_text_vector(
    user_id: Optional[str],
    query_text: str,
    top_k: int = 50,
) -> List[Dict]:
    """
    è·¯å¾„1: æ–‡æœ¬å‘é‡æœç´¢
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æŸ¥è¯¢æ–‡æœ¬
        top_k: å¬å›æ•°é‡
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼ˆåŒ…å« similarity å­—æ®µï¼‰
    """
    try:
        from .embed import embed_text
        query_vec = await embed_text(query_text)
        if not query_vec:
            return []
        
        results = await search_by_text_embedding(
            user_id=user_id,
            query_embedding=query_vec,
            top_k=top_k,
            threshold=MIN_SIMILARITY_THRESHOLD,  # ä½¿ç”¨é…ç½®çš„æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¿‡æ»¤å®Œå…¨ä¸ç›¸å…³çš„ç»“æœ
        )
        
        # æ·»åŠ è·¯å¾„æ ‡è¯†
        for item in results:
            item["recall_path"] = "text_vector"
            item["text_similarity"] = item.get("similarity", 0.0)
        
        print(f"[Funnel] Text vector recall: found {len(results)} results for user_id={user_id}")
        return results
    except Exception as e:
        print(f"[Funnel] Error in text vector recall: {e}")
        import traceback
        traceback.print_exc()
        return []


async def _coarse_recall_image_vector(
    user_id: Optional[str],
    query_image_url: Optional[str] = None,
    query_image_base64: Optional[str] = None,
    top_k: int = 50,
) -> List[Dict]:
    """
    è·¯å¾„2: å›¾åƒå‘é‡æœç´¢
    
    Args:
        user_id: ç”¨æˆ·ID
        query_image_url: æŸ¥è¯¢å›¾åƒURL
        query_image_base64: æŸ¥è¯¢å›¾åƒBase64
        top_k: å¬å›æ•°é‡
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼ˆåŒ…å« similarity å­—æ®µï¼‰
    """
    try:
        from .embed import embed_image
        query_vec = await embed_image(query_image_url, query_image_base64)
        if not query_vec:
            return []
        
        results = await search_by_image_embedding(
            user_id=user_id,
            query_embedding=query_vec,
            top_k=top_k,
            threshold=IMAGE_EMBEDDING_THRESHOLD,  # Image embedding ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼ˆ15%ï¼‰ï¼Œå› ä¸ºç»“æœè´¨é‡å¥½
        )
        
        # æ·»åŠ è·¯å¾„æ ‡è¯†
        for item in results:
            item["recall_path"] = "image_vector"
            item["image_similarity"] = item.get("similarity", 0.0)
        
        return results
    except Exception as e:
        print(f"[Funnel] Error in image vector recall: {e}")
        return []


async def _coarse_recall_text_to_image_vector(
    user_id: Optional[str],
    query_text: str,
    top_k: int = 60,
) -> List[Dict]:
    """
    è·¯å¾„2b: æ–‡æœ¬â†’å›¾åƒ å‘é‡æœç´¢ï¼ˆMulti-modalï¼‰
    
    ä½¿ç”¨ç»Ÿä¸€çš„ qwen2.5-vl-embedding å‘é‡ç©ºé—´ï¼š
    - å°†æ–‡æœ¬æŸ¥è¯¢ embed æˆå‘é‡
    - åœ¨ image_embedding åˆ—ä¸Šåšç›¸ä¼¼åº¦æœç´¢ï¼ˆæ–‡æœ¬æœå›¾ï¼‰
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æ–‡æœ¬æŸ¥è¯¢
        top_k: å¬å›æ•°é‡
    """
    try:
        from .embed import embed_text
        query_vec = await embed_text(query_text)
        if not query_vec:
            return []
        
        results = await search_by_image_embedding(
            user_id=user_id,
            query_embedding=query_vec,
            top_k=top_k,
            threshold=IMAGE_EMBEDDING_THRESHOLD,  # Textâ†’Image ä¹Ÿä½¿ç”¨ image embedding é˜ˆå€¼ï¼ˆ15%ï¼‰ï¼Œæ›´å®½æ¾
        )
        
        for item in results:
            item["recall_path"] = "text_to_image_vector"
            # è¿™ä¸€è·¯æœ¬è´¨ä¸Šæ˜¯ image ç›¸ä¼¼åº¦ï¼Œåªæ˜¯ query æ¥è‡ªæ–‡æœ¬
            item["image_similarity"] = item.get("similarity", 0.0)
        
        print(f"[Funnel] Textâ†’Image vector recall: found {len(results)} results for user_id={user_id}")
        return results
    except Exception as e:
        print(f"[Funnel] Error in text-to-image vector recall: {e}")
        import traceback
        traceback.print_exc()
        return []


async def _coarse_recall_caption_embedding(
    user_id: Optional[str],
    query_text: str,
    top_k: int = 60,
) -> List[Dict]:
    """
    è·¯å¾„2a: Caption Embedding å‘é‡æœç´¢ï¼ˆè¯­ä¹‰æœç´¢ï¼‰
    
    ä½¿ç”¨ç»Ÿä¸€çš„ qwen2.5-vl-embedding å‘é‡ç©ºé—´ï¼š
    - å°†æŸ¥è¯¢æ–‡æœ¬ embed æˆå‘é‡
    - åœ¨ caption_embedding åˆ—ä¸Šåšç›¸ä¼¼åº¦æœç´¢ï¼ˆCaptionè¯­ä¹‰æœç´¢ï¼‰
    
    ä¼˜åŠ¿ï¼šæ¯”å…³é”®è¯æœç´¢æ›´æ™ºèƒ½ï¼Œèƒ½ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆå¦‚"æ¤…å­"å¯ä»¥åŒ¹é…"chair"ã€"seating"ç­‰ï¼‰
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æ–‡æœ¬æŸ¥è¯¢
        top_k: å¬å›æ•°é‡
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼ˆåŒ…å« similarity å­—æ®µï¼‰
    """
    try:
        from .embed import embed_text
        query_vec = await embed_text(query_text)
        if not query_vec:
            return []
        
        results = await search_by_caption_embedding(
            user_id=user_id,
            query_embedding=query_vec,
            top_k=top_k,
            threshold=IMAGE_EMBEDDING_THRESHOLD,  # ä½¿ç”¨ image embedding é˜ˆå€¼ï¼ˆ24%ï¼‰ï¼Œæ›´å®½æ¾
        )
        
        # æ·»åŠ è·¯å¾„æ ‡è¯†
        for item in results:
            item["recall_path"] = "caption_embedding"
            item["caption_similarity"] = item.get("similarity", 0.0)
        
        print(f"[Funnel] Caption embedding recall: found {len(results)} results for user_id={user_id}")
        return results
    except Exception as e:
        print(f"[Funnel] Error in caption embedding recall: {e}")
        import traceback
        traceback.print_exc()
        return []


async def _coarse_recall_caption_keyword(
    user_id: Optional[str],
    query_text: str,
    top_k: int = 50,
) -> List[Dict]:
    """
    è·¯å¾„2b: Caption å…³é”®è¯æœç´¢ï¼ˆå…¨æ–‡æœç´¢ï¼‰
    
    âœ… ä½¿ç”¨ jieba åˆ†è¯å™¨ä¼˜åŒ–ä¸­æ–‡æŸ¥è¯¢
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æŸ¥è¯¢æ–‡æœ¬
        top_k: å¬å›æ•°é‡
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        pool = await get_pool()
        normalized_user = _normalize_user_id(user_id)
        
        # âœ… ä½¿ç”¨ jieba åˆ†è¯å™¨å¤„ç†ä¸­æ–‡æŸ¥è¯¢
        keywords = []
        try:
            import jieba
            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡æŸ¥è¯¢ï¼ˆåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼‰
            has_chinese = any('\u4e00' <= ch <= '\u9fff' for ch in query_text)
            
            if has_chinese:
                # ä¸­æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨ jieba åˆ†è¯
                # æœç´¢æ¨¡å¼ï¼šé€‚åˆæœç´¢åœºæ™¯ï¼Œä¼šåˆ†è¯æ›´ç»†
                keywords = list(jieba.cut_for_search(query_text))
                # è¿‡æ»¤æ‰åœç”¨è¯å’Œå•å­—ç¬¦ï¼ˆé™¤éæ˜¯æŸ¥è¯¢æœ¬èº«ï¼‰
                keywords = [kw.strip() for kw in keywords if len(kw.strip()) > 1 or kw.strip() == query_text.strip()]
                keywords = list(set(keywords))  # å»é‡
                
                if keywords:
                    print(f"[Funnel] Jieba segmented query '{query_text}' â†’ {keywords}")
                else:
                    # å¦‚æœåˆ†è¯å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æŸ¥è¯¢
                    keywords = [query_text]
            else:
                # è‹±æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨ç©ºæ ¼åˆ†è¯
                keywords = query_text.lower().split()
        except ImportError:
            # jieba æœªå®‰è£…ï¼Œå›é€€åˆ°ç®€å•åˆ†è¯
            print("[Funnel] Warning: jieba not installed, falling back to simple tokenization")
            keywords = query_text.split()
        
        # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
        if not keywords:
            keywords = [query_text]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡/å¤šè¯­è¨€æŸ¥è¯¢
        from string import ascii_letters, digits
        def _has_non_ascii(s: str) -> bool:
            allowed = set(ascii_letters + digits + " _-")
            return any(ch not in allowed for ch in s)
        
        use_simple = _has_non_ascii(query_text)
        ts_config = "simple" if use_simple else "english"
        
        async with pool.acquire() as conn:
            # æ£€æŸ¥æ˜¯å¦æœ‰ image_caption å­—æ®µ
            has_caption_field = await conn.fetchval(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.columns 
                    WHERE table_schema = '{NAMESPACE}'
                      AND table_name = 'opengraph_items_v2'
                      AND column_name = 'image_caption'
                );
            """)
            
            if not has_caption_field:
                # é™çº§åˆ° metadata æŸ¥è¯¢
                # âœ… ä½¿ç”¨ jieba åˆ†è¯åçš„å…³é”®è¯è¿›è¡Œ ILIKE æŸ¥è¯¢ï¼ˆå‚æ•°åŒ–ï¼Œé˜²æ­¢ SQL æ³¨å…¥ï¼‰
                keyword_conditions_list = []
                params = [normalized_user]
                param_idx = 2
                
                for kw in keywords:
                    keyword_conditions_list.append(f"metadata->>'caption' ILIKE '%' || ${param_idx} || '%'")
                    params.append(kw)
                    param_idx += 1
                
                keyword_conditions_sql = " OR ".join(keyword_conditions_list)
                
                # å®Œå…¨åŒ¹é…åŸå§‹æŸ¥è¯¢çš„ä¼˜å…ˆçº§æ›´é«˜
                full_match_idx = param_idx
                full_match_condition = f"metadata->>'caption' ILIKE '%' || ${full_match_idx} || '%'"
                params.append(query_text)
                param_idx += 1

                # LIMIT ä½¿ç”¨å•ç‹¬çš„å‚æ•°ç´¢å¼•
                limit_idx = param_idx
                params.append(top_k)
                param_idx += 1
                
                rows = await conn.fetch(f"""
                    SELECT user_id, url, title, description, image, site_name,
                           tab_id, tab_title, metadata,
                           CASE 
                               WHEN {full_match_condition} THEN 1.0
                               ELSE 0.5
                           END AS rank
                    FROM {ACTIVE_TABLE}
                    WHERE status = 'active'
                      AND user_id = $1
                      AND metadata ? 'caption'
                      AND metadata->>'caption' IS NOT NULL
                      AND metadata->>'caption' != ''
                      AND ({keyword_conditions_sql})
                    ORDER BY rank DESC, metadata->>'caption'
                    LIMIT ${limit_idx};
                """, *params)
            else:
                # âœ… ä½¿ç”¨ jieba åˆ†è¯åçš„å…³é”®è¯è¿›è¡Œ ILIKE æŸ¥è¯¢ï¼ˆå‚æ•°åŒ–ï¼Œé˜²æ­¢ SQL æ³¨å…¥ï¼‰
                keyword_conditions_list = []
                params = [normalized_user]
                param_idx = 2
                
                for kw in keywords:
                    keyword_conditions_list.append(f"image_caption ILIKE '%' || ${param_idx} || '%'")
                    params.append(kw)
                    param_idx += 1
                
                keyword_conditions_sql = " OR ".join(keyword_conditions_list)
                
                # å®Œå…¨åŒ¹é…åŸå§‹æŸ¥è¯¢çš„ä¼˜å…ˆçº§æ›´é«˜
                full_match_idx = param_idx
                full_match_condition = f"image_caption ILIKE '%' || ${full_match_idx} || '%'"
                params.append(query_text)
                param_idx += 1

                # LIMIT ä½¿ç”¨å•ç‹¬çš„å‚æ•°ç´¢å¼•
                limit_idx = param_idx
                params.append(top_k)
                param_idx += 1
                
                rows = await conn.fetch(f"""
                    SELECT user_id, url, title, description, image, site_name,
                           tab_id, tab_title, metadata,
                           image_caption, caption_embedding, dominant_colors, style_tags, object_tags,
                           CASE 
                               WHEN {full_match_condition} THEN 1.0
                               ELSE 0.5
                           END AS rank
                    FROM {ACTIVE_TABLE}
                    WHERE status = 'active'
                      AND user_id = $1
                      AND image_caption IS NOT NULL
                      AND image_caption != ''
                      AND ({keyword_conditions_sql})
                    ORDER BY rank DESC, image_caption
                    LIMIT ${limit_idx};
                """, *params)
            
            # ç»Ÿä¸€å¤„ç†ä¸¤ä¸ªåˆ†æ”¯çš„ç»“æœï¼Œåº”ç”¨ rank é˜ˆå€¼è¿‡æ»¤
            results = []
            for row in rows:
                item = dict(row)
                # å½’ä¸€åŒ– rank åˆ° [0, 1]ï¼ˆç¡®ä¿è½¬æ¢ä¸º floatï¼‰
                rank = item.get("rank", 0.0)
                caption_similarity = min(float(rank), 1.0)
                
                # Caption å…³é”®è¯è·¯å¾„ï¼šä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼ˆ60%ï¼‰ï¼Œè¿‡æ»¤æ‰ rank < 0.6 çš„ç»“æœ
                # rank ä¸º 0.5ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰æˆ– 1.0ï¼ˆå®Œå…¨åŒ¹é…ï¼‰ï¼Œæ‰€ä»¥ 0.6 é˜ˆå€¼ä¼šè¿‡æ»¤æ‰éƒ¨åˆ†åŒ¹é…çš„ç»“æœ
                if caption_similarity < CAPTION_RANK_THRESHOLD:
                    continue  # è·³è¿‡ rank å¤ªä½çš„ç»“æœ
                
                item["caption_similarity"] = caption_similarity
                item["recall_path"] = "caption_keyword"
                results.append(item)
            
            print(f"[Funnel] Caption keyword recall: found {len(results)} results (after rank threshold >= {CAPTION_RANK_THRESHOLD}) for user_id={user_id}")
            return results
    except Exception as e:
        print(f"[Funnel] Error in caption keyword recall: {e}")
        import traceback
        traceback.print_exc()
        return []


async def _coarse_recall_designer_sites(
    user_id: Optional[str],
    query_text: str,
    top_k: int = 100,
) -> List[Dict]:
    """
    è·¯å¾„5: è®¾è®¡å¸ˆç½‘ç«™ä¸“é—¨å¬å›ï¼ˆå°çº¢ä¹¦ã€Pinterestã€Behanceç­‰ï¼‰
    
    ä¸“é—¨é’ˆå¯¹è®¾è®¡å¸ˆç½‘ç«™è¿›è¡Œå‘é‡æœç´¢ï¼Œç¡®ä¿å°½å¯èƒ½å¤šåœ°å¬å›è¿™äº›ç½‘ç«™çš„å†…å®¹
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æŸ¥è¯¢æ–‡æœ¬
        top_k: å¬å›æ•°é‡ï¼ˆé»˜è®¤100ï¼Œæ¯”å…¶ä»–è·¯å¾„æ›´å¤šï¼‰
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        from .embed import embed_text
        query_vec = await embed_text(query_text)
        if not query_vec:
            return []
        
        pool = await get_pool()
        normalized_user = _normalize_user_id(user_id)
        query_vec_str = to_vector_str(query_vec)
        
        # è®¾è®¡å¸ˆç½‘ç«™åˆ—è¡¨
        designer_sites = [
            "pinterest.com", "xiaohongshu.com", "behance.net", "dribbble.com",
            "zcool.com.cn", "ui.cn", "uisdc.com", "youzhan.com",
            "unsplash.com", "pexels.com", "pixabay.com", "freepik.com",
            "shutterstock.com", "gettyimages.com", "deviantart.com",
            "artstation.com", "500px.com", "flickr.com", "imgur.com",
            "tumblr.com", "arena.com", "muzli.com", "designspiration.com",
            "awwwards.com", "siteinspire.com", "land-book.com",
            "onepagelove.com", "collectui.com", "mobbin.com",
        ]
        
        async with pool.acquire() as conn:
            # ä¼˜å…ˆä½¿ç”¨ image_embeddingï¼ˆè®¾è®¡å¸ˆç½‘ç«™ä¸»è¦æ˜¯å›¾ç‰‡ï¼‰
            # ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼Œé¿å… SQL æ³¨å…¥
            # æ„å»º LIKE æ¡ä»¶åˆ—è¡¨
            site_like_conditions = []
            params = [query_vec_str, normalized_user, IMAGE_EMBEDDING_THRESHOLD]  # $1, $2, $3 (è®¾è®¡å¸ˆç½‘ç«™ä¸»è¦ç”¨ image embeddingï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼)
            param_idx = 4  # ä» $4 å¼€å§‹
            
            for site in designer_sites:
                site_like_conditions.append(f"url LIKE ${param_idx}")
                params.append(f"%{site}%")
                param_idx += 1
            
            site_conditions_sql = " OR ".join(site_like_conditions)
            
            query_sql = f"""
                SELECT user_id, url, title, description, image, site_name,
                       tab_id, tab_title, text_embedding, image_embedding, metadata,
                       image_caption, caption_embedding, dominant_colors, style_tags, object_tags,
                       CASE 
                           WHEN image_embedding IS NOT NULL THEN 
                               1 - (image_embedding <=> $1::vector(1024))
                           ELSE 
                               1 - (text_embedding <=> $1::vector(1024))
                       END AS similarity
                FROM {ACTIVE_TABLE}
                WHERE status = 'active'
                  AND user_id = $2
                  AND ({site_conditions_sql})
                  AND (
                      (image_embedding IS NOT NULL AND 1 - (image_embedding <=> $1::vector(1024)) >= $3)
                      OR 
                      (text_embedding IS NOT NULL AND 1 - (text_embedding <=> $1::vector(1024)) >= $3)
                  )
                ORDER BY similarity DESC
                LIMIT ${param_idx};
            """
            
            params.append(top_k)  # æ·»åŠ  LIMIT å‚æ•°
            
            rows = await conn.fetch(query_sql, *params)  # threshold=0.0ï¼Œå°½å¯èƒ½å¤šå¬å›
            
            results = []
            for row in rows:
                item = _row_to_dict(row)
                item["recall_path"] = "designer_sites"
                item["designer_site_boost"] = True
                results.append(item)
            
            print(f"[Funnel] Designer sites recall: found {len(results)} results for user_id={user_id}")
            return results
    except Exception as e:
        print(f"[Funnel] Error in designer sites recall: {e}")
        import traceback
        traceback.print_exc()
        return []


async def _coarse_recall_visual_attributes(
    user_id: Optional[str],
    query_text: str,
    top_k: int = 50,
) -> List[Dict]:
    """
    è·¯å¾„4: é¢œè‰²/é£æ ¼æ ‡ç­¾æœç´¢
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æŸ¥è¯¢æ–‡æœ¬
        top_k: å¬å›æ•°é‡
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        # æå–è§†è§‰å±æ€§ï¼ˆé¢œè‰²ã€é£æ ¼ï¼‰
        visual_attrs = enhance_visual_query(query_text)
        colors = visual_attrs.get("colors", [])
        styles = visual_attrs.get("styles", [])
        
        if not colors and not styles:
            return []
        
        pool = await get_pool()
        normalized_user = _normalize_user_id(user_id)
        
        async with pool.acquire() as conn:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å­—æ®µ
            has_new_fields = await conn.fetchval(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.columns 
                    WHERE table_schema = '{NAMESPACE}'
                      AND table_name = 'opengraph_items_v2'
                      AND column_name = 'dominant_colors'
                );
            """)
            
            if not has_new_fields:
                return []
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = []
            params = [normalized_user]
            param_idx = 2
            
            if colors:
                # âœ… å¢å¼ºé¢œè‰²åŒ¹é…ï¼šæ”¯æŒåŒä¹‰è¯åŒ¹é…
                # ä¾‹å¦‚ï¼šæŸ¥è¯¢"é»„è‰²"æ—¶ï¼Œåº”è¯¥åŒ¹é… "yellow", "gold", "amber", "lemon" ç­‰
                color_conditions = []
                
                # é¢œè‰²åŒä¹‰è¯æ˜ å°„ï¼ˆç¡®ä¿æ‰€æœ‰åŒä¹‰è¯éƒ½èƒ½åŒ¹é…ï¼‰
                COLOR_SYNONYMS = {
                    "yellow": ["yellow", "gold", "amber", "lemon", "golden"],
                    "gold": ["yellow", "gold", "amber", "lemon", "golden"],
                    "amber": ["yellow", "gold", "amber", "lemon", "golden"],
                    "lemon": ["yellow", "gold", "amber", "lemon", "golden"],
                    "blue": ["blue", "azure", "navy", "cobalt", "sky blue"],
                    "azure": ["blue", "azure", "navy", "cobalt", "sky blue"],
                    "navy": ["blue", "azure", "navy", "cobalt", "sky blue"],
                    "red": ["red", "crimson", "scarlet", "burgundy"],
                    "crimson": ["red", "crimson", "scarlet", "burgundy"],
                    "green": ["green", "emerald", "olive", "lime"],
                    "emerald": ["green", "emerald", "olive", "lime"],
                    "orange": ["orange", "tangerine", "coral"],
                    "purple": ["purple", "violet", "lavender", "plum"],
                    "pink": ["pink", "rose", "blush", "magenta"],
                }
                
                # æ”¶é›†æ‰€æœ‰éœ€è¦åŒ¹é…çš„é¢œè‰²ï¼ˆåŒ…æ‹¬åŒä¹‰è¯ï¼‰
                all_colors_to_match = set()
                for color in colors:
                    color_lower = color.lower()
                    all_colors_to_match.add(color_lower)
                    # æ·»åŠ åŒä¹‰è¯
                    if color_lower in COLOR_SYNONYMS:
                        all_colors_to_match.update(COLOR_SYNONYMS[color_lower])
                
                # ä¸ºæ¯ä¸ªé¢œè‰²ï¼ˆåŒ…æ‹¬åŒä¹‰è¯ï¼‰åˆ›å»ºåŒ¹é…æ¡ä»¶
                for color_to_match in all_colors_to_match:
                    color_conditions.append(f"${param_idx} = ANY(dominant_colors)")
                    params.append(color_to_match)
                    param_idx += 1
                
                if color_conditions:
                    conditions.append(f"({' OR '.join(color_conditions)})")
            
            if styles:
                # é£æ ¼åŒ¹é…
                style_conditions = []
                for style in styles:
                    style_conditions.append(f"${param_idx} = ANY(style_tags)")
                    params.append(style.lower())
                    param_idx += 1
                conditions.append(f"({' OR '.join(style_conditions)})")
            
            if not conditions:
                return []
            
            where_clause = " AND ".join(conditions)
            params.append(top_k)
            
            # âœ… ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨æ•°ç»„äº¤é›†æ“ä½œç¬¦è®¡ç®— visual_score
            # å¦‚æœ dominant_colors ä¸æŸ¥è¯¢é¢œè‰²æœ‰äº¤é›†ï¼Œç»™é«˜åˆ†
            if colors:
                # ä½¿ç”¨ all_colors_to_matchï¼ˆåŒ…å«åŒä¹‰è¯ï¼‰æ„å»ºæ•°ç»„
                color_array_str = ','.join([f"'{c}'" for c in all_colors_to_match])
                color_score_case = f"WHEN dominant_colors && ARRAY[{color_array_str}]::TEXT[] THEN 0.7 ELSE 0.0"
            else:
                color_score_case = "0.0"
            
            if styles:
                style_array_str = ','.join([f"'{s.lower()}'" for s in styles])
                style_score_case = f"WHEN style_tags && ARRAY[{style_array_str}]::TEXT[] THEN 0.3 ELSE 0.0"
            else:
                style_score_case = "0.0"
            
            query = f"""
                SELECT user_id, url, title, description, image, site_name,
                       tab_id, tab_title, metadata,
                       image_caption, caption_embedding, dominant_colors, style_tags, object_tags,
                       CASE {color_score_case} END +
                       CASE {style_score_case} END AS visual_score
                FROM {ACTIVE_TABLE}
                WHERE status = 'active'
                  AND user_id = $1
                  AND ({where_clause})
                ORDER BY visual_score DESC
                LIMIT ${param_idx};
            """
            
            rows = await conn.fetch(query, *params)
            
            results = []
            for row in rows:
                item = dict(row)
                # ç¡®ä¿è½¬æ¢ä¸º float
                visual_score = item.get("visual_score", 0.0)
                item["visual_attributes_score"] = float(visual_score) if visual_score is not None else 0.0
                item["recall_path"] = "visual_attributes"
                results.append(item)
            
            print(f"[Funnel] Visual attributes recall: found {len(results)} results for user_id={user_id}")
            return results
    except Exception as e:
        print(f"[Funnel] Error in visual attributes recall: {e}")
        import traceback
        traceback.print_exc()
        return []


async def _fine_ranking(
    candidates: List[Dict],
    query_text: str,
    query_text_vec: Optional[List[float]] = None,
    query_image_vec: Optional[List[float]] = None,
) -> List[Dict]:
    """
    ç²¾æ’åºï¼šèåˆ 5 è·¯åˆ†æ•° + ä¸¥æ ¼è¿‡æ»¤ä¸ç›¸å…³ç»“æœ
    
    äº”è·¯åˆ†æ•°ï¼š
    1. æ–‡æœ¬ç›¸ä¼¼åº¦ (15%)
    2. å›¾åƒç›¸ä¼¼åº¦ (35%)
    3. Caption ç›¸ä¼¼åº¦ (20%)
    4. å…³é”®è¯åŒ¹é… (15%)
    5. è§†è§‰å±æ€§åŒ¹é… (15%)
    
    è¿‡æ»¤æœºåˆ¶ï¼š
    1. æ ‡ç­¾åŒ¹é…åº¦æ£€æŸ¥ï¼ˆå¦‚æœæŸ¥è¯¢æœ‰æ˜ç¡®çš„è§†è§‰å±æ€§ï¼Œç»“æœå¿…é¡»æœ‰åŒ¹é…çš„æ ‡ç­¾ï¼‰
    2. æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆèåˆåˆ†æ•°å¤ªä½ç›´æ¥è¿‡æ»¤ï¼‰
    3. çº¯è‰²å›¾ç‰‡/ä»£ç æˆªå›¾æ£€æµ‹ï¼ˆé€šè¿‡æ ‡é¢˜/æè¿°åˆ¤æ–­ï¼‰
    
    Args:
        candidates: å€™é€‰ç»“æœåˆ—è¡¨
        query_text: æŸ¥è¯¢æ–‡æœ¬
        query_text_vec: æŸ¥è¯¢æ–‡æœ¬å‘é‡ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›ä¼šè®¡ç®—ï¼‰
        query_image_vec: æŸ¥è¯¢å›¾åƒå‘é‡ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ’åºåçš„ç»“æœåˆ—è¡¨ï¼ˆåŒ…å« similarity å­—æ®µï¼‰
    """
    if not candidates:
        return []
    
    # æå–æŸ¥è¯¢çš„è§†è§‰å±æ€§ï¼Œç”¨äºç²¾æ’åºé˜¶æ®µçš„æ ‡ç­¾åŒ¹é…æ£€æŸ¥
    from .query_enhance import enhance_visual_query
    visual_attrs = enhance_visual_query(query_text)
    query_colors = [c.lower() for c in visual_attrs.get("colors", [])]
    query_objects = [o.lower() for o in visual_attrs.get("objects", [])]
    query_styles = [s.lower() for s in visual_attrs.get("styles", [])]
    
    # ç²¾æ’åºé˜¶æ®µçš„æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆåªè¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„ç»“æœï¼‰
    MIN_FINE_RANKING_SIMILARITY = 0.10  # 10%ï¼Œé™ä½é˜ˆå€¼ï¼Œé¿å…è¯¯è¿‡æ»¤ç›¸å…³ç»“æœ
    
    # å¦‚æœæœªæä¾›æ–‡æœ¬å‘é‡ï¼Œè®¡ç®—å®ƒ
    if query_text_vec is None and query_text:
        try:
            query_text_vec = await embed_text(query_text)
        except Exception as e:
            print(f"[Funnel] Error computing query text vector: {e}")
            query_text_vec = None
    
    # åˆå¹¶ç»“æœï¼ˆå»é‡ï¼‰
    merged = {}  # url -> item
    
    for item in candidates:
        url = item.get("url")
        if not url:
            continue
        
        if url not in merged:
            merged[url] = {
                "text_similarity": 0.0,
                "image_similarity": 0.0,
                "caption_similarity": 0.0,
                "keyword_score": 0.0,
                "visual_attributes_score": 0.0,
                "recall_paths": set(),
            }
            # å¤åˆ¶åŸå§‹å­—æ®µ
            for key, value in item.items():
                if key not in ["text_similarity", "image_similarity", "caption_similarity", 
                              "keyword_score", "visual_attributes_score", "similarity", 
                              "recall_path", "recall_paths"]:
                    merged[url][key] = value
        
        # åˆå¹¶åˆ†æ•°ï¼ˆç¡®ä¿è½¬æ¢ä¸º floatï¼‰
        merged[url]["text_similarity"] = max(
            merged[url]["text_similarity"],
            float(item.get("text_similarity", 0.0) or 0.0)
        )
        merged[url]["image_similarity"] = max(
            merged[url]["image_similarity"],
            float(item.get("image_similarity", 0.0) or 0.0)
        )
        merged[url]["caption_similarity"] = max(
            merged[url]["caption_similarity"],
            float(item.get("caption_similarity", 0.0) or 0.0)
        )
        merged[url]["visual_attributes_score"] = max(
            merged[url]["visual_attributes_score"],
            float(item.get("visual_attributes_score", 0.0) or 0.0)
        )
        merged[url]["recall_paths"].add(item.get("recall_path", "unknown"))
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        title = item.get("title", "")
        description = item.get("description", "")
        # å¤„ç† metadataï¼ˆå¯èƒ½æ˜¯ dict æˆ– strï¼‰
        metadata = item.get("metadata", {})
        if isinstance(metadata, str):
            import json
            try:
                metadata = json.loads(metadata)
            except:
                metadata = {}
        caption = item.get("image_caption") or (metadata.get("caption", "") if isinstance(metadata, dict) else "")
        
        keyword_score = fuzzy_score(query_text, title, f"{description} {caption}")
        merged[url]["keyword_score"] = max(merged[url]["keyword_score"], float(keyword_score))
    
    # è®¡ç®—èåˆåˆ†æ•° + ä¸¥æ ¼è¿‡æ»¤
    results = []
    filtered_count = 0
    filter_reasons = {
        "low_similarity": 0,
        "tag_mismatch": 0,
        "no_tags": 0,
        "irrelevant_content": 0,
    }
    
    for url, item in merged.items():
        # å¦‚æœç¼ºå°‘å‘é‡ç›¸ä¼¼åº¦ï¼Œå°è¯•è®¡ç®—
        if query_text_vec and item.get("text_embedding") and item["text_similarity"] == 0.0:
            try:
                text_vec = item.get("text_embedding")
                # âœ… ä¿®å¤ï¼šå¤„ç† text_embedding å¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ
                if isinstance(text_vec, str):
                    # å°è¯•è§£æå­—ç¬¦ä¸²ï¼ˆå¯èƒ½æ˜¯ JSON æ ¼å¼çš„æ•°ç»„ï¼‰
                    import json
                    try:
                        text_vec = json.loads(text_vec)
                    except:
                        # å¦‚æœä¸æ˜¯ JSONï¼Œå¯èƒ½æ˜¯ PostgreSQL æ•°ç»„æ ¼å¼ï¼Œè·³è¿‡
                        text_vec = None
                
                if isinstance(text_vec, list) and len(text_vec) > 0:
                    item["text_similarity"] = max(
                        item["text_similarity"],
                        cosine_similarity(query_text_vec, text_vec)
                    )
            except Exception as e:
                print(f"[Funnel] Error computing text similarity for {url}: {e}")
        
        if query_image_vec and item.get("image_embedding") and item["image_similarity"] == 0.0:
            try:
                image_vec = item.get("image_embedding")
                # âœ… ä¿®å¤ï¼šå¤„ç† image_embedding å¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ
                if isinstance(image_vec, str):
                    import json
                    try:
                        image_vec = json.loads(image_vec)
                    except:
                        image_vec = None
                
                if isinstance(image_vec, list) and len(image_vec) > 0:
                    item["image_similarity"] = max(
                        item["image_similarity"],
                        cosine_similarity(query_image_vec, image_vec)
                    )
            except Exception as e:
                print(f"[Funnel] Error computing image similarity for {url}: {e}")
        
        # èåˆäº”è·¯åˆ†æ•°
        weights = FUSION_WEIGHTS
        fused_score = (
            weights["text_similarity"] * item["text_similarity"] +
            weights["image_similarity"] * item["image_similarity"] +
            weights["caption_similarity"] * item["caption_similarity"] +
            weights["keyword_match"] * item["keyword_score"] +
            weights["visual_attributes"] * item["visual_attributes_score"]
        )
        
        item["similarity"] = fused_score
        
        # ========== ç²¾æ’åºé˜¶æ®µæ™ºèƒ½è¿‡æ»¤ ==========
        # ç­–ç•¥ï¼šåªè¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„ç»“æœï¼Œä¿ç•™æœ‰åŒ¹é…æ ‡ç­¾çš„ç»“æœ
        should_filter = False
        filter_reason = ""
        
        # å…ˆæ£€æŸ¥æ ‡ç­¾åŒ¹é…æƒ…å†µï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™ï¼‰
        item_colors = item.get("dominant_colors", []) or []
        item_objects = item.get("object_tags", []) or []
        item_styles = item.get("style_tags", []) or []
        
        # è½¬æ¢ä¸ºå°å†™åˆ—è¡¨
        if isinstance(item_colors, list):
            item_colors = [str(c).lower() for c in item_colors if c]
        else:
            item_colors = []
        
        if isinstance(item_objects, list):
            item_objects = [str(o).lower() for o in item_objects if o]
        else:
            item_objects = []
        
        if isinstance(item_styles, list):
            item_styles = [str(s).lower() for s in item_styles if s]
        else:
            item_styles = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„æ ‡ç­¾ï¼ˆå¦‚æœæœ‰åŒ¹é…ï¼Œåº”è¯¥ä¿ç•™ï¼‰
        has_color_match = any(qc in item_colors for qc in query_colors) if query_colors else False
        has_object_match = any(qo in item_objects for qo in query_objects) if query_objects else False
        has_style_match = any(qs in item_styles for qs in query_styles) if query_styles else False
        has_any_match = has_color_match or has_object_match or has_style_match
        
        # 1. ä¸ç›¸å…³å†…å®¹æ£€æµ‹ï¼ˆçº¯è‰²å›¾ç‰‡ã€ä»£ç æˆªå›¾ç­‰ï¼‰- æœ€ä¸¥æ ¼ï¼Œç›´æ¥è¿‡æ»¤
        title = (item.get("title") or item.get("tab_title") or "").lower()
        description = (item.get("description") or "").lower()
        caption = (item.get("image_caption") or "").lower()
        all_text = f"{title} {description} {caption}".lower()
        
        # æ£€æµ‹çº¯è‰²å›¾ç‰‡å…³é”®è¯ï¼ˆæ’é™¤æŸ¥è¯¢é¢œè‰²æœ¬èº«ï¼Œæ¯”å¦‚æŸ¥è¯¢"ç»¿è‰²"æ—¶ï¼Œ"green"ä¸åº”è¯¥è¢«è¿‡æ»¤ï¼‰
        solid_color_keywords = [
            "solid", "pure", "blank", "çº¯è‰²", "çº¯ç™½", "çº¯é»‘", "çº¯çº¢", "ç©ºç™½", "å•è‰²",
        ]
        # å¦‚æœæ ‡é¢˜æ˜ç¡®è¯´æ˜¯"çº¯è‰²"æˆ–"ç©ºç™½"ï¼Œä¸”èåˆåˆ†æ•°å¾ˆä½ï¼Œè¿‡æ»¤æ‰
        if any(kw in all_text for kw in solid_color_keywords) and fused_score < 0.15:
            should_filter = True
            filter_reason = "irrelevant_content"
        
        # æ£€æµ‹ä»£ç /å¼€å‘ç¯å¢ƒå…³é”®è¯
        code_keywords = [
            "codesandbox", "terminal", "editor", "code", "programming", "git",
            "localhost", "npm", "yarn", "build", "compile", "debug",
            "ä»£ç ", "ç¼–ç¨‹", "å¼€å‘", "ç¼–è¾‘å™¨", "ç»ˆç«¯",
        ]
        if any(kw in all_text for kw in code_keywords) and fused_score < 0.15:
            should_filter = True
            filter_reason = "irrelevant_content"
        
        # 2. å¦‚æœæœ‰åŒ¹é…çš„æ ‡ç­¾ï¼ˆé¢œè‰²/ç‰©ä½“/é£æ ¼ï¼‰ï¼Œå³ä½¿èåˆåˆ†æ•°ç¨ä½ä¹Ÿä¿ç•™
        # ä¾‹å¦‚ï¼šæŸ¥è¯¢"ç»¿è‰²æ¤ç‰©"ï¼Œç»“æœæœ‰"green"é¢œè‰²æ ‡ç­¾ï¼Œåº”è¯¥ä¿ç•™
        if has_any_match:
            # æœ‰åŒ¹é…æ ‡ç­¾çš„ç»“æœï¼Œé™ä½è¿‡æ»¤é˜ˆå€¼ï¼Œæ›´å®½æ¾
            if fused_score < 0.08:  # åªæœ‰éå¸¸ä½çš„åˆ†æ‰è¿‡æ»¤
                should_filter = True
                filter_reason = "low_similarity_with_match"
        else:
            # æ²¡æœ‰åŒ¹é…æ ‡ç­¾çš„ç»“æœï¼Œåº”ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼
            # 3. æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼æ£€æŸ¥ï¼ˆåªå¯¹æ²¡æœ‰åŒ¹é…æ ‡ç­¾çš„ç»“æœåº”ç”¨ï¼‰
            if fused_score < MIN_FINE_RANKING_SIMILARITY:
                should_filter = True
                filter_reason = "low_similarity"
            
            # 4. å¦‚æœæŸ¥è¯¢æœ‰æ˜ç¡®çš„è§†è§‰å±æ€§ï¼Œä½†ç»“æœå®Œå…¨ä¸åŒ¹é…ï¼Œä¸”èåˆåˆ†æ•°å¾ˆä½ï¼Œè¿‡æ»¤æ‰
            if (query_colors or query_objects) and fused_score < 0.15:
                should_filter = True
                filter_reason = "tag_mismatch"
        
        if should_filter:
            filtered_count += 1
            filter_reasons[filter_reason] = filter_reasons.get(filter_reason, 0) + 1
            continue
        
        item["recall_paths"] = list(item["recall_paths"])
        results.append(item)
    
    # æŒ‰èåˆåˆ†æ•°æ’åº
    results.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)
    
    if filtered_count > 0:
        print(f"[Funnel] Fine ranking filtered out {filtered_count} items:")
        for reason, count in filter_reasons.items():
            if count > 0:
                print(f"  - {reason}: {count}")
    
    return results


async def search_with_funnel(
    user_id: Optional[str],
    query_text: str,
    query_image_url: Optional[str] = None,
    query_image_base64: Optional[str] = None,
    filter_mode: FilterMode = FilterMode.BALANCED,
    max_results: Optional[int] = None,  # æ”¹ä¸ºå¯é€‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶æ•°é‡ï¼Œåªæ ¹æ®è´¨é‡è¿‡æ»¤
    use_caption: bool = True,
    filter_urls: Optional[List[str]] = None,  # âœ… æ–°å¢ï¼šåªæœç´¢è¿™äº› URLï¼ˆPersonal Space ä¸­çš„ URLï¼‰
    filter_tab_ids: Optional[List[str]] = None,  # âœ… æ–°å¢ï¼šåªæœç´¢è¿™äº› tab_idï¼ˆPersonal Space ä¸­çš„ tab_idï¼‰
) -> List[Dict]:
    """
    ä¸‰é˜¶æ®µæ¼æ–—æœç´¢
    
    Args:
        user_id: ç”¨æˆ·ID
        query_text: æŸ¥è¯¢æ–‡æœ¬
        query_image_url: æŸ¥è¯¢å›¾åƒURLï¼ˆå¯é€‰ï¼‰
        query_image_base64: æŸ¥è¯¢å›¾åƒBase64ï¼ˆå¯é€‰ï¼‰
        filter_mode: è¿‡æ»¤æ¨¡å¼
        max_results: æœ€å¤§è¿”å›æ•°é‡
        use_caption: æ˜¯å¦ä½¿ç”¨ Caption æœç´¢
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼ˆæ ¹æ®è´¨é‡é˜ˆå€¼åŠ¨æ€è¿”å›ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
    """
    print(f"[Funnel] Starting funnel search for query: {query_text[:50]}...")
    
    # âœ… æ­¥éª¤ 0: AI å¢å¼ºæŸ¥è¯¢ï¼ˆåœ¨æœç´¢å‰å°±ç†è§£ç”¨æˆ·çœŸå®æ„å›¾ï¼‰
    enhanced_query = query_text
    ai_enhanced = False
    enhanced_intent = None
    
    try:
        from .ai_intent_enhance import hybrid_intent_detection
        # ä½¿ç”¨æ··åˆæ„å›¾æ£€æµ‹ï¼ˆè§„åˆ™å¼ + AIï¼Œè¶…æ—¶3ç§’ï¼Œç»™AIæ›´å¤šæ—¶é—´ï¼‰
        enhanced_intent = await hybrid_intent_detection(
            query_text,
            use_ai=True,
            ai_timeout=3.0,  # å¢åŠ åˆ°3ç§’ï¼Œè®©AIæœ‰æ›´å¤šæ—¶é—´åˆ†æ
            cache={}  # å¯ä»¥ä¼ å…¥å¤–éƒ¨ç¼“å­˜
        )
        
        if enhanced_intent and enhanced_intent.get("ai_enhanced"):
            enhanced_query = enhanced_intent.get("enhanced_query", query_text)
            ai_enhanced = True
            print(f"[Funnel] âœ… AI enhanced query: '{query_text}' â†’ '{enhanced_query}'")
            print(f"[Funnel]    Query type: {enhanced_intent.get('query_type')}")
            print(f"[Funnel]    Extracted: colors={enhanced_intent.get('extracted_info', {}).get('colors', [])}, "
                  f"objects={enhanced_intent.get('extracted_info', {}).get('objects', [])}, "
                  f"styles={enhanced_intent.get('extracted_info', {}).get('styles', [])}")
        else:
            print(f"[Funnel] âš ï¸  AI enhancement not available, using original query")
    except Exception as e:
        print(f"[Funnel] âš ï¸  AI enhancement failed: {e}, using original query")
        import traceback
        traceback.print_exc()
    
    # ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢è¿›è¡Œæœç´¢ï¼ˆå¦‚æœAIå¢å¼ºæˆåŠŸï¼Œä½¿ç”¨å¢å¼ºæŸ¥è¯¢ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼‰
    search_query = enhanced_query if ai_enhanced else query_text
    
    # ========== é˜¶æ®µ 1: ç²—å¬å›ï¼ˆMulti-Recallï¼‰ ==========
    print("[Funnel] Stage 1: Coarse Recall (Multi-Recall)")
    print(f"[Funnel] Using {'AI-enhanced' if ai_enhanced else 'original'} query: '{search_query[:50]}...'")
    print("[Funnel] Priority order: Image Vector / Textâ†’Image > Caption Embedding > Caption Keyword > Visual Attributes > Designer Sites > Text Vector")
    
    recall_tasks = []
    
    # âœ… ä¼˜å…ˆçº§1: å›¾åƒå‘é‡æœç´¢ï¼ˆæœ‰å›¾åƒæŸ¥è¯¢æ—¶ï¼‰
    if query_image_url or query_image_base64:
        recall_tasks.append(_coarse_recall_image_vector(
            user_id, query_image_url, query_image_base64, top_k=80
        ))
    
    # âœ… ä¼˜å…ˆçº§1b: æ–‡æœ¬â†’å›¾åƒå‘é‡æœç´¢ï¼ˆå¤šæ¨¡æ€æ–‡æœ¬æœå›¾ï¼Œå§‹ç»ˆå¼€å¯ï¼‰
    # ä½¿ç”¨AIå¢å¼ºåçš„æŸ¥è¯¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if search_query:
        recall_tasks.append(_coarse_recall_text_to_image_vector(
            user_id, search_query, top_k=80  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
        ))
    
    # âœ… æ£€æµ‹æ˜¯å¦æ˜¯é¢œè‰²æŸ¥è¯¢
    from .query_enhance import enhance_visual_query
    visual_attrs = enhance_visual_query(query_text)
    is_color_query = len(visual_attrs.get("colors", [])) > 0
    
    # âœ… å¦‚æœæ˜¯é¢œè‰²æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨è§†è§‰å±æ€§æœç´¢
    if is_color_query:
        print(f"[Funnel] ğŸ¨ Color query detected: {visual_attrs.get('colors')}, prioritizing visual attributes search")
        # âœ… ä¼˜å…ˆçº§1c: è§†è§‰å±æ€§æœç´¢ï¼ˆé¢œè‰²æŸ¥è¯¢æ—¶ä¼˜å…ˆçº§æœ€é«˜ï¼‰
        recall_tasks.append(_coarse_recall_visual_attributes(user_id, search_query, top_k=100))  # âœ… é¢œè‰²æŸ¥è¯¢æ—¶æé«˜å¬å›æ•°é‡
    
    # âœ… ä¼˜å…ˆçº§2a: Caption Embedding å‘é‡æœç´¢ï¼ˆè¯­ä¹‰æœç´¢ï¼Œæ›´æ™ºèƒ½ï¼‰
    # ä½¿ç”¨AIå¢å¼ºåçš„æŸ¥è¯¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_caption and search_query:
        recall_tasks.append(_coarse_recall_caption_embedding(user_id, search_query, top_k=60))  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
    
    # âœ… ä¼˜å…ˆçº§2b: Caption å…³é”®è¯æœç´¢ï¼ˆå…¨æ–‡æœç´¢ï¼Œä½œä¸ºè¡¥å……ï¼‰
    # åŒæ—¶ä½¿ç”¨åŸå§‹æŸ¥è¯¢å’Œå¢å¼ºæŸ¥è¯¢ï¼Œæé«˜å¬å›ç‡
    if use_caption:
        recall_tasks.append(_coarse_recall_caption_keyword(user_id, search_query, top_k=80))  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
        # å¦‚æœAIå¢å¼ºæˆåŠŸï¼Œä¹Ÿå°è¯•åŸå§‹æŸ¥è¯¢ï¼ˆå¯èƒ½åŒ…å«æ›´ç²¾ç¡®çš„å…³é”®è¯ï¼‰
        if ai_enhanced and search_query != query_text:
            recall_tasks.append(_coarse_recall_caption_keyword(user_id, query_text, top_k=40))  # åŸå§‹æŸ¥è¯¢ä½œä¸ºè¡¥å……
    
    # âœ… ä¼˜å…ˆçº§3: è§†è§‰å±æ€§æœç´¢ï¼ˆéé¢œè‰²æŸ¥è¯¢æ—¶ä½¿ç”¨ï¼Œé¢œè‰²æŸ¥è¯¢æ—¶å·²åœ¨ä¸Šé¢å¤„ç†ï¼‰
    if not is_color_query:
        recall_tasks.append(_coarse_recall_visual_attributes(user_id, search_query, top_k=50))  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
    
    # âœ… ä¼˜å…ˆçº§4: è®¾è®¡å¸ˆç½‘ç«™ä¸“é—¨å¬å›ï¼ˆå°çº¢ä¹¦ã€Pinterestã€Behanceç­‰ï¼‰
    recall_tasks.append(_coarse_recall_designer_sites(user_id, search_query, top_k=100))  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
    
    # âœ… ä¼˜å…ˆçº§5: æ–‡æœ¬å‘é‡æœç´¢ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼Œä½œä¸ºè¡¥å……ï¼‰
    recall_tasks.append(_coarse_recall_text_vector(user_id, search_query, top_k=80))  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¬å›è·¯å¾„
    recall_results = await asyncio.gather(*recall_tasks, return_exceptions=True)
    
    # åˆå¹¶å¬å›ç»“æœ
    all_candidates = []
    for results in recall_results:
        if isinstance(results, Exception):
            print(f"[Funnel] Recall path error: {results}")
            continue
        all_candidates.extend(results)
    
    print(f"[Funnel] Coarse recall: {len(all_candidates)} candidates (before filtering)")
    
    # âœ… ä¿®å¤ï¼šåœ¨ç²—å¬å›é˜¶æ®µå°±è¿‡æ»¤æ–‡æ¡£ç±»å†…å®¹ï¼ˆè®¾è®¡å¸ˆåœºæ™¯ï¼‰
    from .preprocess import is_doc_like
    from .query_enhance import enhance_visual_query
    
    # æå–æŸ¥è¯¢çš„è§†è§‰å±æ€§ï¼ˆé¢œè‰²ã€ç‰©ä½“ã€é£æ ¼ï¼‰ï¼Œç”¨äºæ ‡ç­¾åŒ¹é…è¿‡æ»¤
    visual_attrs = enhance_visual_query(query_text)
    query_colors = [c.lower() for c in visual_attrs.get("colors", [])]
    query_objects = [o.lower() for o in visual_attrs.get("objects", [])]
    query_styles = [s.lower() for s in visual_attrs.get("styles", [])]
    
    doc_count = 0
    tag_mismatch_count = 0
    filtered_candidates = []
    seen_urls = set()  # ç”¨äºå»é‡
    seen_titles = set()  # ç”¨äºå»é‡ç›¸ä¼¼æ ‡é¢˜
    original_count = len(all_candidates)  # è®°å½•åŸå§‹æ•°é‡
    
    for item in all_candidates:
        url = item.get("url", "")
        title = (item.get("title") or item.get("tab_title") or "").strip()
        
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡æ¡£ç±»å†…å®¹
        is_doc = is_doc_like(item)
        
        # æ£€æŸ¥metadataä¸­çš„is_doc_cardæ ‡è®°
        metadata = item.get("metadata", {})
        if isinstance(metadata, str):
            import json
            try:
                metadata = json.loads(metadata)
            except:
                metadata = {}
        
        is_doc_card = metadata.get("is_doc_card", False) or item.get("is_doc_card", False)
        
        if is_doc or is_doc_card:
            doc_count += 1
            continue  # è·³è¿‡æ–‡æ¡£ç±»å†…å®¹
        
        # 2. æ ‡ç­¾åŒ¹é…è¿‡æ»¤ï¼šå¦‚æœæŸ¥è¯¢æœ‰æ˜ç¡®çš„è§†è§‰å±æ€§ï¼ˆé¢œè‰²/ç‰©ä½“/é£æ ¼ï¼‰ï¼Œæ£€æŸ¥ç»“æœæ˜¯å¦åŒ¹é…
        # ä¾‹å¦‚ï¼šæŸ¥è¯¢"æ¤ç‰©"åº”è¯¥è¿”å›æœ‰ "plant" æ ‡ç­¾æˆ– "green" é¢œè‰²çš„ç»“æœ
        # âš ï¸ ç‰¹åˆ«è¯´æ˜ï¼š
        # - å¯¹äºã€Œçº¯é¢œè‰²æŸ¥è¯¢ã€ï¼ˆåªæœ‰é¢œè‰²ï¼Œæ²¡æœ‰ç‰©ä½“/é£æ ¼ï¼‰ï¼Œè¿™é‡Œä¸åš tag mismatch è¿‡æ»¤ï¼Œ
        #   åªåœ¨åé¢ç”¨é¢œè‰²æ ‡ç­¾åšæ’åºï¼Œé¿å…æŠŠæœ¬æ¥æ˜¯é»„è‰²ä½†è¢«è¯†åˆ«æˆ gold/amber çš„å¡ç‰‡å…¨è¿‡æ»¤æ‰ã€‚
        pure_color_query = bool(query_colors) and not query_objects and not query_styles

        if (query_colors or query_objects or query_styles) and not pure_color_query:
            item_colors = item.get("dominant_colors", []) or []
            item_objects = item.get("object_tags", []) or []
            item_styles = item.get("style_tags", []) or []
            
            # è½¬æ¢ä¸ºå°å†™åˆ—è¡¨ï¼ˆå¤„ç†å¯èƒ½æ˜¯å­—ç¬¦ä¸²æ•°ç»„æˆ– None çš„æƒ…å†µï¼‰
            if isinstance(item_colors, list):
                item_colors = [str(c).lower() for c in item_colors if c]
            else:
                item_colors = []
            
            if isinstance(item_objects, list):
                item_objects = [str(o).lower() for o in item_objects if o]
            else:
                item_objects = []
            
            if isinstance(item_styles, list):
                item_styles = [str(s).lower() for s in item_styles if s]
            else:
                item_styles = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„æ ‡ç­¾
            has_color_match = False
            has_object_match = False
            has_style_match = False
            
            if query_colors:
                # âœ… æ”¹è¿›ï¼šä¼˜å…ˆæ£€æŸ¥ä¸»è‰²ï¼ˆç¬¬ä¸€ä¸ªé¢œè‰²ï¼‰ï¼Œé¿å…é»„ç»¿è‰²è¢«è¯¯è¯†åˆ«ä¸ºé»„è‰²
                # å¦‚æœæŸ¥è¯¢çš„æ˜¯é»„è‰²ï¼ŒåªåŒ¹é…ä¸»è‰²æ˜¯é»„è‰²çš„ï¼ˆç¬¬ä¸€ä¸ªé¢œè‰²æ˜¯yellow/gold/amberï¼‰
                has_color_match = False
                
                # æ£€æŸ¥ä¸»è‰²ï¼ˆç¬¬ä¸€ä¸ªé¢œè‰²ï¼‰
                if item_colors and len(item_colors) > 0:
                    primary_color = item_colors[0].lower()
                    
                    # å¯¹äºé»„è‰²æŸ¥è¯¢ï¼Œä¸¥æ ¼åŒ¹é…ï¼šä¸»è‰²å¿…é¡»æ˜¯é»„è‰²ç³»åˆ—
                    if any(qc.lower() in ["yellow", "gold", "amber", "lemon", "golden"] for qc in query_colors):
                        yellow_keywords = ["yellow", "gold", "amber", "lemon", "golden"]
                        if any(kw in primary_color for kw in yellow_keywords):
                            has_color_match = True
                        # å¦‚æœä¸»è‰²æ˜¯ç»¿è‰²ç³»åˆ—ï¼Œæ˜ç¡®æ’é™¤ï¼ˆé¿å…é»„ç»¿è‰²è¢«è¯¯è¯†åˆ«ï¼‰
                        elif any(kw in primary_color for kw in ["green", "emerald", "olive", "lime", "forestgreen"]):
                            has_color_match = False
                    # âœ… æ–°å¢ï¼šå¯¹äºçº¢è‰²æŸ¥è¯¢ï¼Œä¸¥æ ¼åŒ¹é…ï¼šä¸»è‰²å¿…é¡»æ˜¯çº¢è‰²ç³»åˆ—
                    elif any(qc.lower() in ["red", "crimson", "scarlet", "burgundy"] for qc in query_colors):
                        red_keywords = ["red", "crimson", "scarlet", "burgundy"]
                        if any(kw in primary_color for kw in red_keywords):
                            has_color_match = True
                        # å¦‚æœä¸»è‰²æ˜¯é»„è‰²ç³»åˆ—ï¼Œæ˜ç¡®æ’é™¤ï¼ˆé¿å…é»„è‰²è¢«è¯¯è¯†åˆ«ä¸ºçº¢è‰²ï¼‰
                        elif any(kw in primary_color for kw in ["yellow", "gold", "amber", "lemon", "golden"]):
                            has_color_match = False
                    else:
                        # å…¶ä»–é¢œè‰²æŸ¥è¯¢ï¼šæ£€æŸ¥ä¸»è‰²æˆ–ä»»ä½•é¢œè‰²
                        has_color_match = any(qc.lower() in primary_color for qc in query_colors) or \
                                        any(qc.lower() in item_colors for qc in query_colors)
                
                # âœ… æ–°å¢ï¼šå¦‚æœ dominant_colors ä¸­æ²¡æœ‰åŒ¹é…ï¼Œæ£€æŸ¥ Caption ä¸­æ˜¯å¦åŒ…å«é¢œè‰²ç›¸å…³è¯æ±‡
                if not has_color_match:
                    caption = (item.get("image_caption") or "").lower()
                    title = (item.get("title") or "").lower()
                    description = (item.get("description") or "").lower()
                    text_content = f"{title} {description} {caption}"
                    
                    # é¢œè‰²å…³é”®è¯æ˜ å°„ï¼ˆä¸­è‹±æ–‡ï¼‰
                    color_keywords_map = {
                        "yellow": ["yellow", "gold", "amber", "lemon", "golden", "é»„è‰²", "é‡‘è‰²", "é‡‘é»„", "æŸ æª¬é»„"],
                        "gold": ["yellow", "gold", "amber", "lemon", "golden", "é‡‘è‰²", "é‡‘é»„"],
                        "amber": ["yellow", "gold", "amber", "lemon", "golden", "ç¥ç€", "é»„è‰²"],
                        "red": ["red", "crimson", "scarlet", "burgundy", "çº¢è‰²", "çº¢", "èµ¤"],
                        "blue": ["blue", "azure", "navy", "cobalt", "è“è‰²", "è“", "å¤©è“", "æµ·å†›è“"],
                        "green": ["green", "emerald", "olive", "lime", "ç»¿è‰²", "ç»¿", "ç¿ ç»¿"],
                        "orange": ["orange", "tangerine", "coral", "æ©™è‰²", "æ©™", "æ©˜"],
                        "purple": ["purple", "violet", "lavender", "ç´«è‰²", "ç´«", "ç´«ç½—å…°"],
                        "pink": ["pink", "rose", "blush", "magenta", "ç²‰è‰²", "ç²‰", "ç«ç‘°"],
                        "black": ["black", "dark", "ebony", "é»‘è‰²", "é»‘"],
                        "white": ["white", "ivory", "snow", "ç™½è‰²", "ç™½"],
                        "gray": ["gray", "grey", "silver", "charcoal", "ç°è‰²", "ç°"],
                        "brown": ["brown", "saddlebrown", "sienna", "tan", "æ£•è‰²", "æ£•", "è¤è‰²"],
                    }
                    
                    # æ£€æŸ¥æŸ¥è¯¢é¢œè‰²å¯¹åº”çš„å…³é”®è¯æ˜¯å¦åœ¨æ–‡æœ¬ä¸­
                    # âœ… æ”¹è¿›ï¼šåŒæ—¶æ£€æŸ¥æ˜¯å¦æœ‰å†²çªé¢œè‰²çš„å…³é”®è¯ï¼Œå¦‚æœæœ‰åˆ™æ’é™¤
                    for qc in query_colors:
                        qc_lower = qc.lower()
                        if qc_lower in color_keywords_map:
                            keywords = color_keywords_map[qc_lower]
                            
                            # âœ… æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å†²çªé¢œè‰²çš„å…³é”®è¯
                            # å¦‚æœæŸ¥è¯¢çº¢è‰²ï¼Œä½†æ–‡æœ¬ä¸­åŒ…å«é»„è‰²å…³é”®è¯ï¼Œåˆ™ä¸åŒ¹é…
                            conflict_keywords = []
                            if qc_lower in ["red", "crimson", "scarlet", "burgundy"]:
                                conflict_keywords = ["yellow", "gold", "amber", "lemon", "golden", "é»„è‰²", "é‡‘è‰²", "é‡‘é»„"]
                            elif qc_lower in ["yellow", "gold", "amber", "lemon"]:
                                conflict_keywords = ["red", "crimson", "scarlet", "burgundy", "çº¢è‰²", "çº¢", "èµ¤"]
                            
                            # å¦‚æœæ–‡æœ¬ä¸­åŒ…å«å†²çªé¢œè‰²çš„å…³é”®è¯ï¼Œæ˜ç¡®æ’é™¤
                            if conflict_keywords and any(kw in text_content for kw in conflict_keywords):
                                print(f"[Funnel] âŒ Captionå†²çªæ£€æµ‹: æŸ¥è¯¢é¢œè‰² '{qc}' ä½†æ–‡æœ¬ä¸­åŒ…å«å†²çªé¢œè‰²å…³é”®è¯ï¼Œæ’é™¤")
                                has_color_match = False
                                break
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„å…³é”®è¯
                            if any(kw in text_content for kw in keywords):
                                has_color_match = True
                                print(f"[Funnel] âœ… Captionè¾…åŠ©åŒ¹é…: æŸ¥è¯¢é¢œè‰² '{qc}' åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ç›¸å…³è¯æ±‡")
                                break
            
            if query_objects:
                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰©ä½“åŒ¹é…ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªæŸ¥è¯¢ç‰©ä½“åœ¨ç»“æœç‰©ä½“ä¸­ï¼‰
                has_object_match = any(qo in item_objects for qo in query_objects)
            
            if query_styles:
                # æ£€æŸ¥æ˜¯å¦æœ‰é£æ ¼åŒ¹é…ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªæŸ¥è¯¢é£æ ¼åœ¨ç»“æœé£æ ¼ä¸­ï¼‰
                has_style_match = any(qs in item_styles for qs in query_styles)
            
            # âœ… ç®€åŒ–ï¼šåªæ£€æŸ¥æ˜æ˜¾å†²çªçš„æƒ…å†µï¼Œä¸å†ä¸¥æ ¼è¿‡æ»¤
            # å¯¹äºç‰©ä½“/é£æ ¼æŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰æ ‡ç­¾åŒ¹é…ï¼Œä¸åº”è¯¥ç›´æ¥è¿‡æ»¤ï¼Œè€Œæ˜¯ä¿ç•™è®© AI åˆ¤æ–­
            # åªå¯¹æ˜æ˜¾å†²çªçš„é¢œè‰²è¿›è¡Œè¿‡æ»¤
            COLOR_CONFLICTS = {
                "green": ["red", "crimson", "scarlet", "burgundy"],
                "emerald": ["red", "crimson", "scarlet", "burgundy"],
                "olive": ["red", "crimson", "scarlet", "burgundy"],
                "lime": ["red", "crimson", "scarlet", "burgundy"],
                "red": ["green", "emerald", "olive", "lime", "yellow", "gold", "amber", "lemon"],  # âœ… æ–°å¢ï¼šçº¢è‰²ä¸é»„è‰²å†²çª
                "crimson": ["green", "emerald", "olive", "lime", "yellow", "gold", "amber", "lemon"],  # âœ… æ–°å¢ï¼šçº¢è‰²ä¸é»„è‰²å†²çª
                "scarlet": ["green", "emerald", "olive", "lime", "yellow", "gold", "amber", "lemon"],  # âœ… æ–°å¢ï¼šçº¢è‰²ä¸é»„è‰²å†²çª
                "burgundy": ["green", "emerald", "olive", "lime", "yellow", "gold", "amber", "lemon"],  # âœ… æ–°å¢ï¼šçº¢è‰²ä¸é»„è‰²å†²çª
                "blue": ["orange", "tangerine", "coral"],
                "azure": ["orange", "tangerine", "coral"],
                "navy": ["orange", "tangerine", "coral"],
                "cobalt": ["orange", "tangerine", "coral"],
                "orange": ["blue", "azure", "navy", "cobalt"],
                "tangerine": ["blue", "azure", "navy", "cobalt"],
                "coral": ["blue", "azure", "navy", "cobalt"],
                "yellow": ["purple", "violet", "lavender", "red", "crimson", "scarlet", "burgundy"],  # âœ… æ–°å¢ï¼šé»„è‰²ä¸çº¢è‰²å†²çª
                "gold": ["purple", "violet", "lavender", "red", "crimson", "scarlet", "burgundy"],  # âœ… æ–°å¢ï¼šé»„è‰²ä¸çº¢è‰²å†²çª
                "amber": ["purple", "violet", "lavender", "red", "crimson", "scarlet", "burgundy"],  # âœ… æ–°å¢ï¼šé»„è‰²ä¸çº¢è‰²å†²çª
                "lemon": ["purple", "violet", "lavender", "red", "crimson", "scarlet", "burgundy"],  # âœ… æ–°å¢ï¼šé»„è‰²ä¸çº¢è‰²å†²çª
                "purple": ["yellow", "gold", "amber", "lemon"],
                "violet": ["yellow", "gold", "amber", "lemon"],
                "lavender": ["yellow", "gold", "amber", "lemon"],
            }
            
            has_color_conflict = False
            if query_colors and item_colors:
                # æ£€æŸ¥æŸ¥è¯¢é¢œè‰²æ˜¯å¦æœ‰å†²çªè‰²ï¼Œä»¥åŠç»“æœæ˜¯å¦åŒ…å«å†²çªè‰²
                for qc in query_colors:
                    conflicts = COLOR_CONFLICTS.get(qc, [])
                    if any(conflict in item_colors for conflict in conflicts):
                        has_color_conflict = True
                        break
            
            # å¦‚æœæŸ¥è¯¢æœ‰æ˜ç¡®çš„è§†è§‰å±æ€§ï¼Œä½†ç»“æœå®Œå…¨ä¸åŒ¹é…ï¼Œåˆ™è¿‡æ»¤æ‰
            # è§„åˆ™1ï¼šå¦‚æœæŸ¥è¯¢æ˜ç¡®æŒ‡å®šäº†é¢œè‰²ï¼Œä½†ç»“æœæ˜¯å†²çªé¢œè‰²ï¼Œç›´æ¥è¿‡æ»¤æ‰ï¼ˆæœ€ä¸¥æ ¼ï¼‰
            # è§„åˆ™2ï¼šå¦‚æœæŸ¥è¯¢æœ‰ç‰©ä½“/é¢œè‰²/é£æ ¼ï¼Œç»“æœå¿…é¡»è‡³å°‘åŒ¹é…å…¶ä¸­ä¸€ç§
            should_filter = False
            filter_reason = ""
            
            # âœ… ç®€åŒ–è¿‡æ»¤é€»è¾‘ï¼šåªè¿‡æ»¤æ˜æ˜¾å†²çªçš„æƒ…å†µ
            # å¯¹äºç‰©ä½“æŸ¥è¯¢ï¼ˆå¦‚"æ¤…å­"ï¼‰å’Œé¢œè‰²+ç‰©ä½“/é£æ ¼æ··åˆæŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰æ ‡ç­¾åŒ¹é…ï¼Œä¸åº”è¯¥ç›´æ¥è¿‡æ»¤ï¼Œè€Œæ˜¯ä¿ç•™è®© AI åˆ¤æ–­
            if has_color_conflict:
                # é¢œè‰²å†²çªï¼šæŸ¥è¯¢æ˜¯ç»¿è‰²ï¼Œä½†ç»“æœæ˜¯çº¢è‰²ï¼ˆæœ€ä¸¥æ ¼ï¼Œç›´æ¥è¿‡æ»¤ï¼‰
                should_filter = True
                filter_reason = "color_conflict"
            # âœ… æ”¾å®½ï¼šå¯¹äºç‰©ä½“æŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰æ ‡ç­¾åŒ¹é…ï¼Œä¿ç•™è®© AI åˆ¤æ–­ï¼ˆä¸å†è¿‡æ»¤ï¼‰
            # elif query_objects and not has_object_match and not has_color_match:
            #     should_filter = True
            #     filter_reason = "object_mismatch"
            elif query_colors and item_colors and not has_color_match and not has_object_match:
                # å¦‚æœæŸ¥è¯¢æ˜ç¡®æŒ‡å®šäº†é¢œè‰²ï¼ˆå¦‚"ç»¿è‰²"ï¼‰ï¼Œä¸”ç»“æœæœ‰é¢œè‰²æ ‡ç­¾ä½†ä¸æ˜¯æŸ¥è¯¢é¢œè‰²ï¼Œä¹Ÿæ²¡æœ‰ç›¸å…³ç‰©ä½“ï¼Œè¿‡æ»¤æ‰
                # æ³¨æ„ï¼šå¦‚æœç»“æœæ²¡æœ‰é¢œè‰²æ ‡ç­¾ï¼Œæš‚æ—¶ä¸è¿‡æ»¤ï¼ˆå¯èƒ½æ ‡ç­¾æœªç”Ÿæˆï¼‰
                should_filter = True
                filter_reason = "color_mismatch_with_tags"
            # âœ… æ”¾å®½ï¼šå¯¹äºé¢œè‰²æŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰é¢œè‰²æ ‡ç­¾ï¼Œä¿ç•™è®© AI åˆ¤æ–­ï¼ˆä¸å†è¿‡æ»¤ï¼‰
            # elif query_colors and not item_colors and not has_object_match:
            #     should_filter = True
            #     filter_reason = "color_mismatch_no_tags"
            # âœ… æ”¾å®½ï¼šå¯¹äºé£æ ¼æŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰é£æ ¼åŒ¹é…ï¼Œä¿ç•™è®© AI åˆ¤æ–­ï¼ˆä¸å†è¿‡æ»¤ï¼‰
            # elif query_styles and not has_style_match and not has_color_match and not has_object_match:
            #     should_filter = True
            #     filter_reason = "style_mismatch"
            
            if should_filter:
                tag_mismatch_count += 1
                continue
        
        # 3. å»é‡ï¼šåŸºäº URLï¼ˆæ ‡å‡†åŒ– URLï¼Œç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹ï¼‰
        normalized_url = None
        if url:
            # æ ‡å‡†åŒ– URLï¼ˆç§»é™¤æŸ¥è¯¢å‚æ•°ã€é”šç‚¹ã€å°¾éšæ–œæ ï¼‰
            try:
                from urllib.parse import urlparse, urlunparse
                parsed = urlparse(url)
                normalized_url = urlunparse((
                    parsed.scheme,
                    parsed.netloc,
                    parsed.path.rstrip('/'),
                    '',  # params
                    '',  # query - ç§»é™¤æŸ¥è¯¢å‚æ•°
                    ''   # fragment - ç§»é™¤é”šç‚¹
                )).lower()
            except:
                normalized_url = url.lower()
            
            if normalized_url in seen_urls:
                continue  # è·³è¿‡é‡å¤çš„ URL
            
            seen_urls.add(normalized_url)
        
        # 4. å»é‡ï¼šåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆè¿‡æ»¤é‡å¤çš„å‘¨ä¼šè®°å½•ã€å·¥ä½œå°ã€å°çº¢ä¹¦ä¸»é¡µç­‰ï¼‰
        if title:
            # æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºæ¯”è¾ƒï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€ç©ºæ ¼ã€æ•°å­—IDç­‰ï¼‰
            import re
            # ç§»é™¤æ•°å­—IDï¼ˆå¦‚ _-1451008, _73823749, _45390241 ç­‰ï¼‰
            title_clean = re.sub(r'[_-]\d+', '', title)
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡ã€ç©ºæ ¼ã€è¿å­—ç¬¦
            normalized_title = "".join(c for c in title_clean if c.isalnum() or c in (' ', '-', '_', 'ï¼Œ', 'ã€‚')).strip().lower()
            
            # ç‰¹æ®Šå¤„ç†ï¼šå°çº¢ä¹¦ä¸»é¡µç­‰é€šç”¨æ ‡é¢˜ï¼ˆå¦‚"å°çº¢ä¹¦_-_ä½ çš„ç”Ÿæ´»å…´è¶£ç¤¾åŒº"ï¼‰
            # å¦‚æœæ ‡é¢˜æ˜¯é€šç”¨æ ‡é¢˜ä¸”URLå·²å­˜åœ¨ï¼Œè·³è¿‡
            generic_titles = [
                "å°çº¢ä¹¦_-_ä½ çš„ç”Ÿæ´»å…´è¶£ç¤¾åŒº",
                "xiaohongshu",
                "pinterest",
                "behance",
                "dribbble",
            ]
            is_generic_title = any(gt in normalized_title for gt in generic_titles)
            
            # å¦‚æœæ ‡é¢˜å¤ªç›¸ä¼¼ï¼ˆæ¯”å¦‚éƒ½æ˜¯"20251117è§†è§‰è®¾è®¡éƒ¨ç®¡ç†å‘¨ä¼š"æˆ–"é¢è¯•å®˜å·¥ä½œå°"ï¼‰ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª
            if normalized_title in seen_titles:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤çš„å‘¨ä¼šè®°å½•ã€å·¥ä½œå°ç­‰ï¼Œæˆ–è€…æ˜¯é€šç”¨æ ‡é¢˜
                if any(kw in normalized_title for kw in ["å‘¨ä¼š", "ä¼šè®®", "ç®¡ç†", "è®¾è®¡éƒ¨", "å·¥ä½œå°", "å‘¨æŠ¥æ”¶çº³"]) or is_generic_title:
                    continue  # è·³è¿‡é‡å¤çš„å‘¨ä¼šè®°å½•ã€å·¥ä½œå°ã€é€šç”¨æ ‡é¢˜ç­‰
            
            seen_titles.add(normalized_title)
        
        filtered_candidates.append(item)
    
    all_candidates = filtered_candidates
    
    if doc_count > 0:
        print(f"[Funnel] Filtered out {doc_count} doc items in coarse recall stage (designer-focused)")
    
    if tag_mismatch_count > 0:
        print(f"[Funnel] Filtered out {tag_mismatch_count} items with tag mismatch (query: colors={query_colors}, objects={query_objects}, styles={query_styles})")
    
    duplicate_count = original_count - len(all_candidates) - doc_count - tag_mismatch_count
    if duplicate_count > 0:
        print(f"[Funnel] Removed {duplicate_count} duplicate items")
    
    print(f"[Funnel] Coarse recall: {len(all_candidates)} unique candidates (after filtering, removed {doc_count} docs + {tag_mismatch_count} tag mismatches + {duplicate_count} duplicates)")
    
    if not all_candidates:
        print("[Funnel] âš ï¸  No candidates found in recall stage")
        print("[Funnel] ğŸ’¡  Possible reasons:")
        print("[Funnel]    1. No data in database for this user")
        print("[Funnel]    2. No embeddings generated")
        print("[Funnel]    3. User ID mismatch")
        print("[Funnel]    4. Query embedding generation failed")
        return []
    
    # âœ… ä¸ºç²—å¬å›ç»“æœç»Ÿä¸€è®¾ç½® similarity å­—æ®µï¼ˆå–å„è·¯åˆ†æ•°çš„æœ€å¤§å€¼ï¼‰
    # è¿™æ · AI ç­›é€‰é˜¶æ®µå¯ä»¥åŸºäº similarity è¿›è¡Œè¿‡æ»¤
    for item in all_candidates:
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„ç›¸ä¼¼åº¦åˆ†æ•°
        scores = []
        if "similarity" in item:
            scores.append(item["similarity"])
        if "text_similarity" in item:
            scores.append(item["text_similarity"])
        if "image_similarity" in item:
            scores.append(item["image_similarity"])
        if "caption_similarity" in item:
            scores.append(item["caption_similarity"])
        
        # ä½¿ç”¨æœ€å¤§åˆ†æ•°ä½œä¸ºç»Ÿä¸€çš„ similarityï¼ˆå¦‚æœå„è·¯éƒ½æœ‰åˆ†æ•°ï¼‰
        if scores:
            item["similarity"] = max(scores)
        elif "similarity" not in item:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼ï¼ˆé¿å… AI ç­›é€‰é˜¶æ®µå‡ºé”™ï¼‰
            item["similarity"] = 0.5  # ä¸­ç­‰åˆ†æ•°ï¼Œè®© AI ç­›é€‰æ¥å†³å®š
    
    # ========== é˜¶æ®µ 2: AIç›‘ç£ç­›é€‰ï¼ˆSmart Filteringï¼‰ ==========
    print(f"[Funnel] Stage 2: AI-Supervised Smart Filtering (mode={filter_mode.value})")
    print("[Funnel] âš ï¸  Fine ranking stage removed (as requested)")
    
    # âœ… ä½¿ç”¨AIå¢å¼ºåçš„æŸ¥è¯¢è¿›è¡Œæ™ºèƒ½è¿‡æ»¤ï¼ˆå¦‚æœAIå¢å¼ºæˆåŠŸï¼‰
    filter_query = search_query if ai_enhanced else query_text
    
    # ä½¿ç”¨AIç›‘ç£ç­›é€‰ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹æ™ºèƒ½è°ƒæ•´è¿‡æ»¤ç­–ç•¥
    # filter_docs=True: é’ˆå¯¹è®¾è®¡å¸ˆåœºæ™¯ï¼Œè‡ªåŠ¨è¿‡æ»¤æ–‡æ¡£ç±»å†…å®¹
    filtered_results = await smart_filter(
        all_candidates,  # âœ… ç›´æ¥ä½¿ç”¨ç²—å¬å›ç»“æœï¼Œè·³è¿‡ç²¾æ’åº
        filter_query,  # âœ… ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
        filter_mode=filter_mode,
        max_results=max_results,
        filter_docs=True,  # è®¾è®¡å¸ˆåœºæ™¯ï¼šè¿‡æ»¤æ–‡æ¡£ç±»å†…å®¹
    )
    
    print(f"[Funnel] Final results: {len(filtered_results)} items")
    
    # âœ… æ–°å¢ï¼šå¦‚æœæä¾›äº† filter_urls æˆ– filter_tab_idsï¼Œåªè¿”å› Personal Space ä¸­çš„ç»“æœ
    if filter_urls or filter_tab_ids:
        original_count = len(filtered_results)
        filter_urls_set = set(filter_urls or [])
        filter_tab_ids_set = set(filter_tab_ids or [])
        
        # è§„èŒƒåŒ– URLï¼ˆç§»é™¤å°¾éšæ–œæ å’ŒæŸ¥è¯¢å‚æ•°ï¼‰
        def normalize_url_for_filter(url: str) -> str:
            if not url:
                return ""
            try:
                from urllib.parse import urlparse
                parsed = urlparse(url)
                normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path.rstrip('/')}"
                return normalized
            except:
                return url
        
        # åˆ›å»ºè§„èŒƒåŒ– URL é›†åˆ
        normalized_filter_urls = set()
        for url in filter_urls_set:
            normalized_filter_urls.add(url)
            normalized = normalize_url_for_filter(url)
            if normalized:
                normalized_filter_urls.add(normalized)
        
        filtered_by_personal_space = []
        for item in filtered_results:
            # ä¼˜å…ˆä½¿ç”¨ tab_id åŒ¹é…
            if filter_tab_ids_set and item.get("tab_id"):
                if str(item.get("tab_id")) in filter_tab_ids_set:
                    filtered_by_personal_space.append(item)
                    continue
            
            # ä½¿ç”¨ URL åŒ¹é…
            item_url = item.get("url", "")
            if item_url:
                if item_url in filter_urls_set or item_url in normalized_filter_urls:
                    filtered_by_personal_space.append(item)
                    continue
                # å°è¯•è§„èŒƒåŒ– URL åŒ¹é…
                normalized_item_url = normalize_url_for_filter(item_url)
                if normalized_item_url in normalized_filter_urls:
                    filtered_by_personal_space.append(item)
                    continue
        
        filtered_results = filtered_by_personal_space
        removed_count = original_count - len(filtered_results)
        if removed_count > 0:
            print(f"[Funnel] âœ… Filtered by Personal Space: {removed_count} items removed, {len(filtered_results)} items remain")
    
    return filtered_results


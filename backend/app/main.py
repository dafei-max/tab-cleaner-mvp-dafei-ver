from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime
import json
import os
# âœ… å·²ç§»é™¤ï¼šä¸å†ä»åç«¯æŠ“å– OpenGraphï¼Œåªæ¥æ”¶å®¢æˆ·ç«¯æ•°æ®
# from opengraph import fetch_multiple_opengraph
from ai_insight import analyze_opengraph_data
from search import process_opengraph_for_search
from clustering import create_manual_cluster, classify_by_labels, discover_clusters
from clustering.storage import save_clustering_result, save_multiple_clusters

app = FastAPI(title="Tab Cleaner MVP", version="0.0.1")


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–å‘é‡æ•°æ®åº“å’Œ Caption å·¥ä½œçº¿ç¨‹"""
    try:
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†æ•°æ®åº“è¿æ¥
        db_host = os.getenv("ADBPG_HOST", "")
        if db_host:
            try:
                from vector_db import init_schema
                print("[Startup] Initializing vector database...")
                await init_schema()
                print("[Startup] âœ“ Vector database initialized successfully")
            except ImportError as import_error:
                print(f"[Startup] âš  Vector DB module import failed: {import_error}")
                print("[Startup] âš  This is expected if asyncpg is not installed. Vector DB features will be disabled.")
                print("[Startup] âš  To enable vector DB, ensure asyncpg is installed: pip install asyncpg>=0.30.0")
            except Exception as db_error:
                print(f"[Startup] âš  Vector DB initialization failed: {db_error}")
                print("[Startup] âš  Continuing without vector database...")
                import traceback
                traceback.print_exc()
        else:
            print("[Startup] ADBPG_HOST not configured, skipping vector database initialization")
        
        # å¯åŠ¨ Caption è‡ªåŠ¨ç”Ÿæˆå·¥ä½œçº¿ç¨‹
        try:
            from search.auto_caption import start_caption_worker
            start_caption_worker()
            print("[Startup] âœ“ Caption worker started")
        except Exception as caption_error:
            print(f"[Startup] âš  Failed to start caption worker: {caption_error}")
            import traceback
            traceback.print_exc()
    except Exception as e:
        print(f"[Startup] âš  Startup event error (non-critical): {e}")
        # ä¸é˜»æ­¢åº”ç”¨å¯åŠ¨


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº"""
    try:
        from vector_db import close_pool
        await close_pool()
        print("[Shutdown] Vector database connection pool closed")
    except Exception as e:
        print(f"[Shutdown] Error closing vector database: {e}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# serve static pages (for share link)
# æ³¨æ„ï¼šå¦‚æœä¸éœ€è¦é™æ€æ–‡ä»¶æœåŠ¡ï¼Œå¯ä»¥åˆ é™¤ static ç›®å½•
static_dir = Path(__file__).parent / "static"
# åªæœ‰å½“ static ç›®å½•å­˜åœ¨ä¸”ä¸ä¸ºç©ºæ—¶æ‰æŒ‚è½½
if static_dir.exists() and static_dir.is_dir():
    try:
        app.mount("/public", StaticFiles(directory=static_dir, html=True), name="public")
    except Exception as e:
        # å¦‚æœæŒ‚è½½å¤±è´¥ï¼ˆä¾‹å¦‚ç›®å½•ä¸ºç©ºï¼‰ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“åº”ç”¨å¯åŠ¨
        print(f"[Warning] Failed to mount static directory: {e}")

@app.get("/")
def root():
    return {"ok": True, "message": "Hello Tab Cleaner"}


@app.get("/favicon.ico")
async def favicon():
    """è¿”å› favicon å›¾æ ‡"""
    # ä¼˜å…ˆå°è¯• .ico æ ¼å¼
    favicon_path = Path(__file__).parent / "static" / "favicon.ico"
    if not favicon_path.exists():
        # å¦‚æœæ²¡æœ‰ .icoï¼Œå°è¯• .png
        favicon_path = Path(__file__).parent / "static" / "favicon.png"
    
    if favicon_path.exists():
        return FileResponse(favicon_path)
    # å¦‚æœæ²¡æœ‰ faviconï¼Œè¿”å› 204 No Content
    from fastapi.responses import Response
    return Response(status_code=204)


# OpenGraph API
class TabItem(BaseModel):
    url: str
    title: Optional[str] = None
    id: Optional[int] = None


class OpenGraphRequest(BaseModel):
    tabs: List[TabItem]
    # å¯é€‰ï¼šå‰ç«¯å·²æŠ“å–çš„ OpenGraph æ•°æ®ï¼ˆç”¨äºéœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
    local_opengraph_data: Optional[List[Dict[str, Any]]] = None


@app.post("/api/v1/tabs/opengraph")
async def fetch_tabs_opengraph(request: OpenGraphRequest):
    """
    æ¥æ”¶å®¢æˆ·ç«¯å‘é€çš„æœ¬åœ° OpenGraph æ•°æ®
    åç«¯ä¸å†ä¸»åŠ¨æŠ“å– OpenGraphï¼Œåªæ¥æ”¶å’Œå¤„ç†å®¢æˆ·ç«¯æ•°æ®
    """
    try:
        print(f"[API] ğŸ“¥ /api/v1/tabs/opengraph endpoint called")
        print(f"[API] Request details: tabs={len(request.tabs)}, local_opengraph_data={len(request.local_opengraph_data) if request.local_opengraph_data else 0}")
        
        # âœ… ç®€åŒ–ï¼šåªæ¥æ”¶å®¢æˆ·ç«¯å‘é€çš„ local_opengraph_data
        if request.local_opengraph_data and len(request.local_opengraph_data) > 0:
            print(f"[API] âœ… Received local OpenGraph data for {len(request.local_opengraph_data)} items")
            
            # æ‰“å°ç¬¬ä¸€ä¸ª item çš„è¯¦ç»†ä¿¡æ¯
            if len(request.local_opengraph_data) > 0:
                first_item = request.local_opengraph_data[0]
                print(f"[API] ğŸ“‹ First local OG item sample:", {
                    "url": first_item.get("url"),
                    "has_title": bool(first_item.get("title")),
                    "has_description": bool(first_item.get("description")),
                    "has_image": bool(first_item.get("image")),
                    "image_preview": str(first_item.get("image"))[:60] + "..." if first_item.get("image") else None,
                    "success": first_item.get("success"),
                    "is_local_fetch": first_item.get("is_local_fetch"),
                })
            
            # åˆ›å»º tab URL åˆ° tab ä¿¡æ¯çš„æ˜ å°„
            tab_map = {tab.url: tab for tab in request.tabs}
            
            # å¤„ç†æœ¬åœ°æ•°æ®ï¼šæ ‡è®°ä¸º is_local_fetch=Trueï¼Œå¹¶åˆå¹¶ tab ä¿¡æ¯
            opengraph_data = []
            for item in request.local_opengraph_data:
                url = item.get("url")
                if not url:
                    continue
                
                tab = tab_map.get(url)
                normalized_item = {
                    **item,
                    "is_local_fetch": True,  # âœ… æ ‡è®°ä¸ºæœ¬åœ°æŠ“å–
                    "tab_id": tab.id if tab else None,
                    "tab_title": tab.title if tab else None,
                }
                opengraph_data.append(normalized_item)
            
            print(f"[API] âœ… Processed {len(opengraph_data)} items from local OpenGraph data")
            return {"ok": True, "data": opengraph_data}
        else:
            # âœ… å¦‚æœæ²¡æœ‰æœ¬åœ°æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶è®°å½•è­¦å‘Š
            print("[API] âš ï¸ No local_opengraph_data provided; backend no longer fetches OG by itself.")
            return {"ok": True, "data": []}
    except Exception as e:
        print(f"[API] âŒ Error processing OpenGraph request: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# AI æ´å¯Ÿ API
class AIInsightRequest(BaseModel):
    opengraph_items: List[Dict[str, Any]]


@app.post("/api/v1/ai/insight")
async def get_ai_insight(request: AIInsightRequest):
    """
    ä½¿ç”¨é€šä¹‰åƒé—®åˆ†æ OpenGraph æ•°æ®å¹¶ç”Ÿæˆæ€»ç»“
    """
    try:
        if not request.opengraph_items:
            raise HTTPException(status_code=400, detail="No OpenGraph items provided")
        
        result = analyze_opengraph_data(request.opengraph_items)
        
        if result["success"]:
            return {
                "ok": True,
                "summary": result["summary"],
                "error": None
            }
        else:
            return {
                "ok": False,
                "summary": None,
                "error": result.get("error", "Unknown error")
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# æœç´¢ API
class EmbeddingRequest(BaseModel):
    opengraph_items: List[Dict[str, Any]]


class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 20
    query_image_url: Optional[str] = None  # âœ… ä»¥å›¾æœå›¾ï¼šæŸ¥è¯¢å›¾ç‰‡ URL
    query_image_base64: Optional[str] = None  # âœ… ä»¥å›¾æœå›¾ï¼šæŸ¥è¯¢å›¾ç‰‡ Base64


@app.post("/api/v1/search/embedding")
async def generate_embeddings(
    request: EmbeddingRequest,
    user_id: Optional[str] = Header(None, alias="X-User-ID")
):
    """
    ä¸ºOpenGraphæ•°æ®ç”ŸæˆEmbeddingå‘é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
    
    æµç¨‹ï¼š
    1. è°ƒç”¨ process_opengraph_for_search() ç”Ÿæˆ text_embedding å’Œ image_embedding
    2. è°ƒç”¨ batch_upsert_items() æ‰¹é‡å­˜å‚¨åˆ°æ•°æ®åº“
    3. è¿”å›åŒ…å« saved å­—æ®µçš„å“åº”
    """
    try:
        normalized_user_id = (user_id or "anonymous").strip() or "anonymous"
        if not request.opengraph_items:
            print("[API] âš ï¸ No opengraph_items provided in request")
            return {"ok": True, "saved": 0, "data": []}
        
        print(f"[API] ğŸ“¥ Received request with {len(request.opengraph_items)} items for embedding generation")
        print(f"[API] ğŸ” Endpoint: /api/v1/search/embedding")
        
        # âœ… æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼šæ‰“å°ç¬¬ä¸€ä¸ª item çš„å­—æ®µ
        if len(request.opengraph_items) > 0:
            first_item = request.opengraph_items[0]
            print(f"[API] ğŸ“‹ First item sample:", {
                "url": first_item.get("url"),
                "has_title": bool(first_item.get("title")),
                "has_description": bool(first_item.get("description")),
                "has_image": bool(first_item.get("image")),
                "image_type": type(first_item.get("image")).__name__ if first_item.get("image") else None,
                "image_preview": str(first_item.get("image"))[:60] + "..." if first_item.get("image") else None,
                "tab_id": first_item.get("tab_id"),
                "is_doc_card": first_item.get("is_doc_card"),
                "success": first_item.get("success"),
                "is_local_fetch": first_item.get("is_local_fetch"),  # âœ… æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æŠ“å–
            })
        
        # âœ… æ­¥éª¤ 0: è§„èŒƒåŒ–è¾“å…¥æ•°æ®
        from search.normalize import normalize_opengraph_items
        normalized_items = normalize_opengraph_items(request.opengraph_items)
        print(f"[API] Normalized {len(normalized_items)} items from {len(request.opengraph_items)} input items")
        
        # âœ… æ­¥éª¤ 0.5: æ£€æŸ¥æ•°æ®åº“ä¸­å·²æœ‰çš„ embeddingï¼ˆè‡ªåŠ¨è¡¥å…¨é€»è¾‘ï¼‰
        from vector_db import get_items_by_urls
        items_already_done = []
        items_to_process = []
        
        db_host = os.getenv("ADBPG_HOST", "")
        if db_host:
            print(f"[API] Checking database for existing embeddings...")
            
            # æ‰¹é‡è·å–æ‰€æœ‰ URL çš„æ•°æ®
            urls = [item.get("url") for item in normalized_items if item.get("url")]
            existing_items_map = {}
            if urls:
                existing_items = await get_items_by_urls(normalized_user_id, urls)
                existing_items_map = {item['url']: item for item in existing_items}
                print(f"[API] Found {len(existing_items)} items in database")
            
            for item in normalized_items:
                url = item.get("url")
                if not url:
                    items_to_process.append(item)
                    continue
                
                # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰å®Œæ•´çš„ embedding
                existing_item = existing_items_map.get(url)
                if existing_item:
                    has_text_emb = existing_item.get("text_embedding") and len(existing_item.get("text_embedding", [])) > 0
                    has_image_emb = existing_item.get("image_embedding") and len(existing_item.get("image_embedding", [])) > 0
                    
                    # å¦‚æœå·²æœ‰å®Œæ•´çš„ embeddingï¼Œç›´æ¥ä½¿ç”¨
                    if has_text_emb and has_image_emb:
                        # åˆå¹¶æ•°æ®ï¼šä½¿ç”¨æ•°æ®åº“ä¸­çš„ embeddingï¼Œä½†ä¿ç•™è¯·æ±‚ä¸­çš„å…¶ä»–å­—æ®µ
                        merged_item = {
                            **item,
                            "text_embedding": existing_item.get("text_embedding"),
                            "image_embedding": existing_item.get("image_embedding"),
                            "has_embedding": True,
                        }
                        items_already_done.append(merged_item)
                        continue
                    # å¦‚æœåªæœ‰éƒ¨åˆ† embeddingï¼Œä¹Ÿä½¿ç”¨å·²æœ‰çš„ï¼Œä½†æ ‡è®°éœ€è¦è¡¥å…¨
                    elif has_text_emb or has_image_emb:
                        merged_item = {
                            **item,
                            "text_embedding": existing_item.get("text_embedding") or item.get("text_embedding"),
                            "image_embedding": existing_item.get("image_embedding") or item.get("image_embedding"),
                            "has_embedding": has_text_emb or has_image_emb,
                        }
                        items_to_process.append(merged_item)  # éœ€è¦è¡¥å…¨ç¼ºå¤±çš„éƒ¨åˆ†
                        continue
                
                # æ•°æ®åº“ä¸­æ²¡æœ‰æˆ–æ²¡æœ‰ embeddingï¼Œéœ€è¦å¤„ç†
                items_to_process.append(item)
            
            print(f"[API] Embedding status: Total={len(normalized_items)}, "
                  f"Already have={len(items_already_done)}, To process={len(items_to_process)}")
        else:
            # æ²¡æœ‰é…ç½®æ•°æ®åº“ï¼Œå…¨éƒ¨éœ€è¦å¤„ç†
            items_to_process = normalized_items
            print(f"[API] ADBPG_HOST not configured, processing all {len(items_to_process)} items")
        
        # 1. åªä¸ºéœ€è¦å¤„ç†çš„é¡¹ç”Ÿæˆ embedding
        enriched_items = []
        if items_to_process:
            print(f"[API] Generating embeddings for {len(items_to_process)} items...")
            enriched_items = await process_opengraph_for_search(items_to_process)
            print(f"[API] Generated embeddings for {len(enriched_items)} items")
        else:
            print(f"[API] All items already have embeddings, skipping generation")
        
        # åˆå¹¶ç»“æœï¼šå·²æœ‰çš„ + æ–°ç”Ÿæˆçš„
        all_enriched_items = items_already_done + enriched_items
        print(f"[API] Total enriched items: {len(all_enriched_items)}")
        
        # 2. å‡†å¤‡æ‰¹é‡å­˜å‚¨çš„æ•°æ®ï¼ˆåªå­˜å‚¨æ–°ç”Ÿæˆçš„ embeddingï¼‰
        items_to_store = []
        for item in enriched_items:  # åªå¤„ç†æ–°ç”Ÿæˆçš„ï¼Œå·²æœ‰çš„ä¸éœ€è¦å†å­˜å‚¨
            # åªå­˜å‚¨æœ‰ embedding çš„é¡¹
            if item.get("text_embedding") or item.get("image_embedding"):
                # ç¡®ä¿ metadata åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
                metadata = item.get("metadata") or {}
                if not isinstance(metadata, dict):
                    metadata = {}
                
                items_to_store.append({
                    "user_id": normalized_user_id,
                    "url": item.get("url"),
                    "title": item.get("title"),
                    "description": item.get("description"),
                    "image": item.get("image"),  # âœ… å·²ç»æ˜¯è§„èŒƒåŒ–åçš„å­—ç¬¦ä¸²
                    "site_name": item.get("site_name"),
                    "tab_id": item.get("tab_id"),
                    "tab_title": item.get("tab_title"),
                    "text_embedding": item.get("text_embedding"),
                    "image_embedding": item.get("image_embedding"),
                    "metadata": {
                        **metadata,
                        "is_screenshot": item.get("is_screenshot", False),
                        "is_doc_card": item.get("is_doc_card", False),
                        "success": item.get("success", False),
                    }
                })
        
        # 3. è°ƒç”¨ batch_upsert_items() å­˜å‚¨åˆ°æ•°æ®åº“
        saved_count = 0
        db_host = os.getenv("ADBPG_HOST", "")
        if db_host and items_to_store:
            try:
                from vector_db import batch_upsert_items
                saved_count = await batch_upsert_items(items_to_store, user_id=normalized_user_id)
                if saved_count > 0:
                    print(f"[API] âœ“ Stored {saved_count}/{len(items_to_store)} items to vector DB")
                    
                    # 4. å¼‚æ­¥è§¦å‘ Caption ç”Ÿæˆä»»åŠ¡ï¼ˆåªå¤„ç†æœ‰å›¾ç‰‡ä¸”æ²¡æœ‰ Caption çš„é¡¹ï¼‰
                    try:
                        from search.auto_caption import batch_enqueue_caption_tasks
                        # è¿‡æ»¤å‡ºéœ€è¦ç”Ÿæˆ Caption çš„é¡¹ï¼ˆæœ‰å›¾ç‰‡ä½†æ²¡æœ‰ Captionï¼‰
                        items_for_caption = [
                            item for item in items_to_store
                            if item.get("image") and not item.get("image_caption")
                        ]
                        if items_for_caption:
                            await batch_enqueue_caption_tasks(
                                normalized_user_id,
                                items_for_caption,
                                max_items=50  # æ¯æ¬¡æœ€å¤šå¤„ç† 50 ä¸ªï¼Œé¿å…é˜Ÿåˆ—è¿‡è½½
                            )
                            print(f"[API] âœ“ Enqueued {len(items_for_caption)} caption generation tasks")
                    except Exception as caption_error:
                        print(f"[API] âš  Failed to enqueue caption tasks: {caption_error}")
                        import traceback
                        traceback.print_exc()
                else:
                    print(f"[API] âš  Failed to store items to vector DB (saved_count=0)")
            except Exception as e:
                print(f"[API] âš  Failed to store embeddings to DB: {e}")
                import traceback
                traceback.print_exc()
        elif not db_host:
            print(f"[API] âš  ADBPG_HOST not configured, skipping database storage")
        elif not items_to_store:
            print(f"[API] âš  No items with embeddings to store")
        
        # 4. æ ¼å¼åŒ–è¿”å›æ•°æ®ï¼ˆåŒ…æ‹¬å·²æœ‰çš„å’Œæ–°ç”Ÿæˆçš„ï¼‰
        result_data = []
        for item in all_enriched_items:
            has_text_emb = item.get("text_embedding") and len(item.get("text_embedding", [])) > 0
            has_image_emb = item.get("image_embedding") and len(item.get("image_embedding", [])) > 0
            has_emb_flag = item.get("has_embedding", False)
            
            if not has_emb_flag and (has_text_emb or has_image_emb):
                has_emb_flag = True
            
            result_data.append({
                "url": item.get("url"),
                "title": item.get("title") or item.get("tab_title", ""),
                "description": item.get("description", ""),
                "image": item.get("image", ""),
                "site_name": item.get("site_name", ""),
                "tab_id": item.get("tab_id"),
                "tab_title": item.get("tab_title"),
                "embedding": None,  # ä¸å†ä½¿ç”¨èåˆ embedding
                "text_embedding": item.get("text_embedding"),
                "image_embedding": item.get("image_embedding"),
                "has_embedding": has_emb_flag,
                "similarity": item.get("similarity")
            })
        
        # 5. è¿”å›åŒ…å« saved å­—æ®µçš„å“åº”
        return {
            "ok": True,
            "saved": saved_count,
            "data": result_data
        }
    except Exception as e:
        print(f"[API] CRITICAL ERROR in generate_embeddings: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        error_detail = f"{type(e).__name__}: {str(e)}"
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/api/v1/search/query")
async def search_content(
    request: SearchRequest,
    user_id: Optional[str] = Header(None, alias="X-User-ID")
):
    """
    æœç´¢ç›¸å…³å†…å®¹ï¼ˆä½¿ç”¨ä¸‰é˜¶æ®µæ¼æ–—æœç´¢ï¼‰
    
    è¯·æ±‚å‚æ•°:
    - query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰
    - top_k: è¿”å›å‰ K ä¸ªç»“æœï¼ˆå¯é€‰ï¼Œé»˜è®¤ 20ï¼Œå®é™…è¿”å›æ•°é‡å¯èƒ½ä¸º 1-20ï¼‰
    
    è¿”å›:
    - æŒ‰ç›¸å…³æ€§æ’åºçš„OpenGraphæ•°æ®åˆ—è¡¨ï¼ˆåŒ…å«similarityåˆ†æ•°ï¼‰
    - ç»“æœæ•°é‡åŠ¨æ€è°ƒæ•´ï¼ˆ1-20 ä¸ªï¼‰ï¼Œæ ¹æ®è´¨é‡æ™ºèƒ½è¿‡æ»¤
    """
    try:
        if not request.query or not request.query.strip():
            raise HTTPException(status_code=400, detail="query parameter is required")
        
        # æ£€æŸ¥æ•°æ®åº“é…ç½®
        db_host = os.getenv("ADBPG_HOST", "")
        if not db_host:
            raise HTTPException(
                status_code=503,
                detail="Vector database not configured. Please set ADBPG_HOST environment variable."
            )
        
        # ä½¿ç”¨ä¸‰é˜¶æ®µæ¼æ–—æœç´¢
        normalized_user_id = (user_id or "anonymous").strip() or "anonymous"
        print(f"[API] Search request: query='{request.query}', user_id='{normalized_user_id}'")
        if request.query_image_url or request.query_image_base64:
            print(f"[API] Image search enabled: query_image_url={bool(request.query_image_url)}, query_image_base64={bool(request.query_image_base64)}")
        
        from search.funnel_search import search_with_funnel
        from search.threshold_filter import FilterMode
        
        # âœ… è°ƒç”¨æ¼æ–—æœç´¢ï¼ˆæ”¯æŒä»¥å›¾æœå›¾ï¼‰
        search_results = await search_with_funnel(
            user_id=normalized_user_id,
            query_text=request.query,
            query_image_url=request.query_image_url,  # âœ… ä»¥å›¾æœå›¾æ”¯æŒ
            query_image_base64=request.query_image_base64,  # âœ… ä»¥å›¾æœå›¾æ”¯æŒ
            filter_mode=FilterMode.BALANCED,  # å¹³è¡¡æ¨¡å¼ï¼šè¿”å›é«˜è´¨é‡å’Œä¸­ç­‰è´¨é‡ç»“æœ
            max_results=None,  # ä¸é™åˆ¶æ•°é‡ï¼Œè¿”å›æ‰€æœ‰ç¬¦åˆè´¨é‡é˜ˆå€¼çš„ç»“æœ
            use_caption=True,  # å¯ç”¨ Caption æœç´¢
        )
        
        if not search_results:
            print(f"[API] No results found for user={normalized_user_id}, query='{request.query}'")
            return {"ok": True, "results": []}
        
        print(f"[API] Found {len(search_results)} results (dynamic filtering)")
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœï¼ˆä¿æŒä¸å‰ç«¯ useSearch å…¼å®¹ï¼‰
        results = []
        for item in search_results:
            results.append({
                "url": item.get("url", ""),
                "title": item.get("title") or item.get("tab_title", ""),
                "description": item.get("description", ""),
                "image": item.get("image", ""),
                "site_name": item.get("site_name", ""),
                "tab_id": item.get("tab_id"),
                "tab_title": item.get("tab_title"),
                "similarity": float(item.get("similarity", 0.0)),
                # âœ… æ·»åŠ è§†è§‰å±æ€§ï¼ˆç”¨äºæŒ‰é¢œè‰²æ’åºï¼‰
                "dominant_colors": item.get("dominant_colors", []),
                "style_tags": item.get("style_tags", []),
                "object_tags": item.get("object_tags", []),
                # æ·»åŠ è´¨é‡æ ‡ç­¾ï¼ˆå¯é€‰ï¼Œå‰ç«¯å¯ä»¥ä½¿ç”¨ï¼‰
                "quality": item.get("quality", "medium"),
                # æ·»åŠ è§†è§‰åŒ¹é…ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                "visual_match": item.get("visual_match", False),
            })
        
        # æ‰“å°ç›¸ä¼¼åº¦èŒƒå›´ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if results:
            similarities = [r.get("similarity", 0.0) for r in results]
            print(f"[API] Similarity range: min={min(similarities):.6f}, max={max(similarities):.6f}, count={len(results)}")
        
        # è¿”å› JSON å“åº”
        return {
            "ok": True,
            "results": results,
            "count": len(results),  # è¿”å›å®é™…ç»“æœæ•°é‡
        }
    except HTTPException:
        raise
    except Exception as e:
        print(f"[API] CRITICAL ERROR in search_content: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        error_detail = f"{type(e).__name__}: {str(e)}"
        raise HTTPException(status_code=500, detail=error_detail)


# èšç±» API
class ManualClusterRequest(BaseModel):
    item_ids: List[str]
    cluster_name: str
    items_data: List[Dict[str, Any]]
    center_x: Optional[float] = 720
    center_y: Optional[float] = 512


class AIClassifyRequest(BaseModel):
    labels: List[str]
    items_data: List[Dict[str, Any]]
    exclude_item_ids: Optional[List[str]] = None


class AIDiscoverRequest(BaseModel):
    items_data: List[Dict[str, Any]]
    exclude_item_ids: Optional[List[str]] = None
    n_clusters: Optional[int] = None


@app.post("/api/v1/clustering/manual")
async def create_manual_cluster_api(request: ManualClusterRequest):
    """
    åˆ›å»ºç”¨æˆ·è‡ªå®šä¹‰èšç±»
    
    è¯·æ±‚å‚æ•°:
    - item_ids: é€‰ä¸­çš„å¡ç‰‡ ID åˆ—è¡¨
    - cluster_name: èšç±»åç§°
    - items_data: æ‰€æœ‰å¡ç‰‡æ•°æ®
    - center_x: èšç±»ä¸­å¿ƒ X åæ ‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 720ï¼‰
    - center_y: èšç±»ä¸­å¿ƒ Y åæ ‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 512ï¼‰
    
    è¿”å›:
    - èšç±»å¯¹è±¡ï¼ŒåŒ…å« id, name, type, items, center, radius ç­‰ä¿¡æ¯
    """
    try:
        if not request.item_ids or not request.cluster_name:
            raise HTTPException(status_code=400, detail="item_ids and cluster_name are required")
        
        cluster = create_manual_cluster(
            item_ids=request.item_ids,
            cluster_name=request.cluster_name,
            items_data=request.items_data,
            center_x=request.center_x or 720,
            center_y=request.center_y or 512,
        )
        
        # ä¿å­˜ç»“æœåˆ°æœ¬åœ°
        try:
            save_clustering_result(cluster, result_type="manual")
        except Exception as save_error:
            print(f"[API] Failed to save clustering result: {save_error}")
        
        return {"ok": True, "cluster": cluster}
    except Exception as e:
        print(f"[API] ERROR in create_manual_cluster: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/clustering/ai-classify")
async def classify_by_labels_api(request: AIClassifyRequest):
    """
    AI æŒ‰æ ‡ç­¾åˆ†ç±»
    
    è¯·æ±‚å‚æ•°:
    - labels: ç”¨æˆ·å®šä¹‰çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆæœ€å¤š3ä¸ªï¼‰
    - items_data: æ‰€æœ‰å¡ç‰‡æ•°æ®ï¼ˆéœ€è¦åŒ…å« text_embedding å’Œ image_embeddingï¼‰
    - exclude_item_ids: è¦æ’é™¤çš„å¡ç‰‡ ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¾‹å¦‚ç”¨æˆ·è‡ªå®šä¹‰èšç±»ä¸­çš„å¡ç‰‡ï¼‰
    
    è¿”å›:
    - åˆ†ç±»ç»“æœï¼ŒåŒ…å«æ¯ä¸ªæ ‡ç­¾å¯¹åº”çš„èšç±»
    """
    try:
        if not request.labels or len(request.labels) == 0:
            raise HTTPException(status_code=400, detail="labels are required")
        
        result = await classify_by_labels(
            labels=request.labels,
            items_data=request.items_data,
            exclude_item_ids=request.exclude_item_ids,
        )
        
        # ä¿å­˜ç»“æœåˆ°æœ¬åœ°
        try:
            save_multiple_clusters(result.get("clusters", []), result_type="ai-classify")
        except Exception as save_error:
            print(f"[API] Failed to save clustering result: {save_error}")
        
        return {"ok": True, **result}
    except Exception as e:
        print(f"[API] ERROR in classify_by_labels: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/v1/tabs/{tab_id}")
async def delete_tab(
    tab_id: str,
    user_id: Optional[str] = Header(None, alias="X-User-ID")
):
    """
    è½¯åˆ é™¤ä¸€ä¸ª tab
    
    Args:
        tab_id: Tab IDï¼ˆå®é™…ä¸Šæ˜¯ URLï¼‰
        user_id: ç”¨æˆ·IDï¼ˆä»è¯·æ±‚å¤´è·å–ï¼‰
    
    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        normalized_user_id = (user_id or "anonymous").strip() or "anonymous"
        
        from vector_db import soft_delete_tab
        
        success = await soft_delete_tab(normalized_user_id, tab_id)
        
        if success:
            return {"ok": True, "message": f"Tab {tab_id[:50]}... deleted successfully"}
        else:
            raise HTTPException(
                status_code=404,
                detail=f"Tab {tab_id[:50]}... not found or already deleted"
            )
    except HTTPException:
        raise
    except Exception as e:
        print(f"[API] Error deleting tab: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/v1/sessions/{session_id}")
async def delete_session(
    session_id: str,
    user_id: Optional[str] = Header(None, alias="X-User-ID")
):
    """
    è½¯åˆ é™¤ä¸€ä¸ª session åŠå…¶ä¸‹çš„æ‰€æœ‰ tabs
    
    Args:
        session_id: Session ID
        user_id: ç”¨æˆ·IDï¼ˆä»è¯·æ±‚å¤´è·å–ï¼‰
    
    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        normalized_user_id = (user_id or "anonymous").strip() or "anonymous"
        
        from vector_db import soft_delete_session_tabs
        
        deleted_count = await soft_delete_session_tabs(normalized_user_id, session_id)
        
        if deleted_count > 0:
            return {
                "ok": True,
                "message": f"Session {session_id} deleted successfully",
                "deleted_tabs": deleted_count
            }
        else:
            raise HTTPException(
                status_code=404,
                detail=f"Session {session_id} not found or already deleted"
            )
    except HTTPException:
        raise
    except Exception as e:
        print(f"[API] Error deleting session: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/clustering/ai-discover")
async def discover_clusters_api(request: AIDiscoverRequest):
    """
    AI è‡ªå‘ç°èšç±»ï¼ˆä½¿ç”¨ K-means å¯¹æ‰€æœ‰å¡ç‰‡è¿›è¡Œæ— ç›‘ç£èšç±»ï¼‰
    
    è¯·æ±‚å‚æ•°:
    - items_data: æ‰€æœ‰å¡ç‰‡æ•°æ®ï¼ˆéœ€è¦åŒ…å« text_embedding å’Œ image_embeddingï¼‰
    - exclude_item_ids: è¦æ’é™¤çš„å¡ç‰‡ ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¾‹å¦‚ç”¨æˆ·è‡ªå®šä¹‰èšç±»ä¸­çš„å¡ç‰‡ï¼‰
    - n_clusters: èšç±»æ•°é‡ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œè‡ªåŠ¨ç¡®å®š3-5ç»„ï¼‰
    
    è¿”å›:
    - èšç±»ç»“æœï¼ŒåŒ…å«æ¯ä¸ªèšç±»çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬ AI ç”Ÿæˆçš„åç§°ï¼‰
    """
    try:
        if not request.items_data:
            raise HTTPException(status_code=400, detail="items_data is required")
        
        result = await discover_clusters(
            items_data=request.items_data,
            exclude_item_ids=request.exclude_item_ids,
            n_clusters=request.n_clusters,
        )
        
        # ä¿å­˜ç»“æœåˆ°æœ¬åœ°
        try:
            save_multiple_clusters(result.get("clusters", []), result_type="ai-discover")
        except Exception as save_error:
            print(f"[API] Failed to save clustering result: {save_error}")
        
        return {"ok": True, **result}
    except Exception as e:
        print(f"[API] ERROR in discover_clusters: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

"""
OpenGraph æŠ“å–å·¥å…·
æ”¯æŒæˆªå›¾åŠŸèƒ½ï¼šå½“ OpenGraph æŠ“å–å¤±è´¥æˆ–è¯†åˆ«ä¸ºæ–‡æ¡£ç±»ç½‘é¡µæ—¶ï¼Œä½¿ç”¨æˆªå›¾
æ”¯æŒé¢„å– Embeddingï¼šä¸€æ—¦ OpenGraph æ•°æ®è§£æå®Œæˆï¼Œç«‹å³è¯·æ±‚ embedding
"""
import httpx
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Tuple
import asyncio
import json
from pathlib import Path

# Screenshot åŠŸèƒ½å·²ç§»é™¤ï¼ˆPlaywright ä½“ç§¯è¿‡å¤§ï¼Œä¸é€‚åˆ Serverlessï¼‰
# æˆªå›¾åŠŸèƒ½ç”±å‰ç«¯ Chrome Extension çš„ chrome.tabs.captureVisibleTab å¤„ç†

def _is_doc_like_url(url: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡æ¡£ç±»ç½‘é¡µ"""
    url_lower = url.lower()
    doc_keywords = [
        "github.com",
        "readthedocs.io",
        "/docs/",
        "developer.",
        "dev.",
        "documentation",
        "wiki",
    ]
    return any(keyword in url_lower for keyword in doc_keywords)


def get_best_image_candidate(soup, response_url: str) -> Tuple[Optional[str], Optional[str]]:
    """
    CleanTab å›¾åƒå†³ç­–æ ‘ï¼šæŒ‰ä¼˜å…ˆçº§æå–æœ€ä½³å›¾ç‰‡ URL
    
    ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼š
    â‘  é¦–å›¾ï¼ˆæ­£æ–‡ç¬¬ä¸€å¼ å¤§å›¾ï¼‰- æœ€å®Œç¾çš„ preview
    â‘¡ OG/Twitter Card å›¾åƒ - å¹³å°æä¾›çš„é¢„è§ˆå›¾
    â‘¢ æˆªå›¾ fallbackï¼ˆç”±å‰ç«¯å¤„ç†ï¼‰
    â‘£ æ–‡æ¡£ç±»å ä½å›¾ï¼ˆç”±å‰ç«¯å¤„ç†ï¼‰
    â‘¤ faviconï¼ˆä»…ç”¨äº corner badgeï¼Œä¸ä½œä¸ºä¸»å›¾ï¼‰
    
    Returns:
        (image_url, source_type): å›¾ç‰‡ URL å’Œæ¥æºç±»å‹
        source_type å¯èƒ½çš„å€¼ï¼š
        - 'first-img': æ­£æ–‡ç¬¬ä¸€ä¸ªå¤§å›¾ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        - 'og:image': OpenGraph å›¾ç‰‡
        - 'twitter:image': Twitter Card å›¾ç‰‡
        - 'itemprop:image': Schema.org itemprop
        - 'link:image_src': <link rel="image_src">
        - None: æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼ˆéœ€è¦æˆªå›¾æˆ–å ä½å›¾ï¼‰
    """
    from urllib.parse import urljoin
    
    # ç¡®ä¿ response_url æ˜¯å­—ç¬¦ä¸²ï¼ˆhttpx.Response.url å¯èƒ½æ˜¯ URL å¯¹è±¡ï¼‰
    response_url_str = str(response_url) if not isinstance(response_url, str) else response_url
    
    # Pinterest ç‰¹æ®Šå¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨ OG å›¾ç‰‡è€Œä¸æ˜¯é¦–å›¾ï¼ˆå› ä¸ºé¦–å›¾å¯èƒ½æ˜¯ç¼©ç•¥å›¾ï¼‰
    response_url_lower = response_url_str.lower()
    is_pinterest_page = "pinterest.com" in response_url_lower
    
    # â‘  é¦–å›¾ï¼ˆæ­£æ–‡ç¬¬ä¸€å¼ å¤§å›¾ï¼‰- æœ€é«˜ä¼˜å…ˆçº§
    # è¿™æ˜¯æœ€å®Œç¾çš„ previewï¼Œå› ä¸ºå®ƒæ˜¯ç”¨æˆ·å®é™…çœ‹åˆ°çš„å†…å®¹
    # æ³¨æ„ï¼šPinterest ç­‰å¹³å°è·³è¿‡é¦–å›¾é€‰æ‹©ï¼Œç›´æ¥ä½¿ç”¨ OG å›¾ç‰‡
    img_tags = soup.find_all('img', src=True)
    if img_tags and not is_pinterest_page:
        exclude_keywords = [
            'icon', 'logo', 'avatar', 'favicon', 'sprite',
            'button', 'arrow', 'badge', 'spinner', 'loader',
            'placeholder', 'blank', 'pixel', 'tracker', 'beacon'
        ]
        
        best_image = None
        best_score = 0
        
        for img in img_tags:
            src = img.get('src', '').strip()
            if not src:
                continue
            
            # è·³è¿‡ data URI å’Œ SVGï¼ˆé€šå¸¸æ˜¯å°å›¾æ ‡ï¼‰
            if src.startswith('data:') or src.endswith('.svg'):
                continue
            
            # è·³è¿‡åŒ…å«æ’é™¤å…³é”®è¯çš„å›¾ç‰‡
            src_lower = src.lower()
            if any(keyword in src_lower for keyword in exclude_keywords):
                continue
            
            # è®¡ç®—å›¾ç‰‡çš„"ä»£è¡¨æ€§"åˆ†æ•°
            score = 0
            
            # ä¼˜å…ˆé€‰æ‹©æœ‰ alt æ–‡æœ¬çš„å›¾ç‰‡ï¼ˆé€šå¸¸æ˜¯å†…å®¹å›¾ç‰‡ï¼‰
            if img.get('alt'):
                score += 10
            
            # ä¼˜å…ˆé€‰æ‹©è¾ƒå¤§çš„å›¾ç‰‡ï¼ˆé€šè¿‡ classã€id ç­‰åˆ¤æ–­ï¼‰
            img_class = img.get('class', [])
            img_id = img.get('id', '')
            class_id_str = ' '.join(img_class) + ' ' + img_id
            class_id_lower = class_id_str.lower()
            
            # å†…å®¹ç›¸å…³çš„å…³é”®è¯åŠ åˆ†
            content_keywords = ['content', 'main', 'article', 'post', 'image', 'photo', 'picture', 'cover', 'hero', 'banner']
            if any(keyword in class_id_lower for keyword in content_keywords):
                score += 5
            
            # ä¼˜å…ˆé€‰æ‹©ç»å¯¹ URL
            if src.startswith(('http://', 'https://')):
                score += 3
            
            # ä¼˜å…ˆé€‰æ‹©å¸¸è§çš„å›¾ç‰‡æ ¼å¼
            if any(ext in src_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                score += 2
            
            # è·³è¿‡æ˜æ˜¾çš„å°å›¾ç‰‡ï¼ˆé€šè¿‡ URL ä¸­çš„å°ºå¯¸å‚æ•°åˆ¤æ–­ï¼‰
            if any(size in src_lower for size in ['16x16', '32x32', '48x48', '64x64', 'w=16', 'w=32', 'h=16', 'h=32']):
                score -= 10
            
            if score > best_score:
                best_score = score
                best_image = src
        
        if best_image:
            # å¤„ç†ç›¸å¯¹ URL
            if best_image.startswith('//'):
                best_image = 'https:' + best_image
            elif not best_image.startswith(('http://', 'https://')):
                best_image = urljoin(response_url_str, best_image)
            return best_image, 'first-img'
    
    # â‘¡ OG/Twitter Card å›¾åƒ - å¹³å°æä¾›çš„é¢„è§ˆå›¾
    # 2.1 OpenGraph å›¾ç‰‡
    og_image = soup.find('meta', property='og:image')
    if og_image and og_image.get('content'):
        url = og_image.get('content').strip()
        if url:
            # å¤„ç†ç›¸å¯¹ URL
            if url.startswith('//'):
                url = 'https:' + url
            elif not url.startswith(('http://', 'https://')):
                url = urljoin(response_url_str, url)
            return url, 'og:image'
    
    # 2.2 Twitter Card å›¾ç‰‡
    twitter_image = soup.find('meta', attrs={'name': 'twitter:image'}) or soup.find('meta', attrs={'property': 'twitter:image'})
    if twitter_image and twitter_image.get('content'):
        url = twitter_image.get('content').strip()
        if url:
            if url.startswith('//'):
                url = 'https:' + url
            elif not url.startswith(('http://', 'https://')):
                url = urljoin(response_url_str, url)
            return url, 'twitter:image'
    
    # 2.3 Schema.org itemprop="image"
    itemprop_image = soup.find('meta', attrs={'itemprop': 'image'})
    if itemprop_image and itemprop_image.get('content'):
        url = itemprop_image.get('content').strip()
        if url:
            if url.startswith('//'):
                url = 'https:' + url
            elif not url.startswith(('http://', 'https://')):
                url = urljoin(response_url_str, url)
            return url, 'itemprop:image'
    
    # 2.4 <link rel="image_src">
    link_image_src = soup.find('link', attrs={'rel': 'image_src'})
    if link_image_src and link_image_src.get('href'):
        url = link_image_src.get('href').strip()
        if url:
            if url.startswith('//'):
                url = 'https:' + url
            elif not url.startswith(('http://', 'https://')):
                url = urljoin(response_url_str, url)
            return url, 'link:image_src'
    
    # â‘¢ æˆªå›¾ fallback - ç”±å‰ç«¯å¤„ç†ï¼ˆchrome.tabs.captureVisibleTabï¼‰
    # â‘£ æ–‡æ¡£ç±»å ä½å›¾ - ç”±å‰ç«¯å¤„ç†
    # â‘¤ favicon - ä»…ç”¨äº corner badgeï¼Œä¸ä½œä¸ºä¸»å›¾
    
    # æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¿”å› Noneï¼ˆå‰ç«¯ä¼šä½¿ç”¨æˆªå›¾æˆ–å ä½å›¾ï¼‰
    return None, None


async def fetch_opengraph(url: str, timeout: float = 10.0) -> Dict:
    """
    æŠ“å–å•ä¸ª URL çš„ OpenGraph æ•°æ®
    
    å¦‚æœ OpenGraph æŠ“å–å¤±è´¥æˆ–è¯†åˆ«ä¸ºæ–‡æ¡£ç±»ç½‘é¡µï¼Œå°†ä½¿ç”¨æ–‡æ¡£å¡ç‰‡ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    
    Args:
        url: è¦æŠ“å–çš„ç½‘é¡µ URL
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        {
            "url": str,
            "title": str,
            "description": str,
            "image": str,  # OpenGraph å›¾ç‰‡ URL æˆ–æˆªå›¾ Base64
            "site_name": str,
            "success": bool,
            "error": Optional[str],
        }
    """
    result = {
        "url": url,
        "title": "",
        "description": "",
        "image": "",
        "site_name": "",
        "success": False,
        "error": None,
        "needs_screenshot": False,  # æ ‡è®°æ˜¯å¦éœ€è¦å‰ç«¯æˆªå›¾
    }

    # ä¼˜å…ˆå°è¯•æŠ“å– OpenGraphï¼ˆæ‰€æœ‰ç½‘é¡µéƒ½å…ˆå°è¯• OpenGraphï¼‰
    # åªæœ‰ OpenGraph å¤±è´¥ä¸”æ˜¯æ–‡æ¡£ç±»æ—¶ï¼Œæ‰ä½¿ç”¨æˆªå›¾/æ–‡æ¡£å¡ç‰‡
    try:
        # æ„å»ºæ›´å®Œæ•´çš„ headersï¼ˆå‚è€ƒæµ‹è¯•è„šæœ¬ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.8,zh-CN;q=0.6",
            "Accept-Encoding": "gzip, deflate, br",
        }
        
        # å°çº¢ä¹¦ç­‰éœ€è¦ Referer
        url_lower = url.lower()
        if "xiaohongshu.com" in url_lower:
            headers["Referer"] = "https://www.xiaohongshu.com/"
            headers["Origin"] = "https://www.xiaohongshu.com"
        
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            
            # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè®°å½•å…³é”®ä¿¡æ¯ï¼ˆç”¨äºå®šä½ç¯å¢ƒ/é£æ§é—®é¢˜ï¼‰
            print(f"[OpenGraph] ====== è¯Šæ–­ä¿¡æ¯å¼€å§‹ ======")
            print(f"[OpenGraph] Request URL: {url}")
            print(f"[OpenGraph] Final URL: {response.url}")
            print(f"[OpenGraph] Status Code: {response.status_code}")
            print(f"[OpenGraph] Response Length: {len(response.text)} bytes")
            
            # è®°å½•è¯·æ±‚ headersï¼ˆç”¨äºå¯¹æ¯”æœ¬åœ°å’Œäº‘ç«¯ï¼‰
            print(f"[OpenGraph] Request Headers:")
            for k, v in headers.items():
                print(f"[OpenGraph]   {k}: {v}")
            
            # è®°å½•å“åº” headersï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰é‡å®šå‘ã€é™åˆ¶ç­‰ï¼‰
            print(f"[OpenGraph] Response Headers (å…³é”®):")
            important_headers = ['content-type', 'content-length', 'location', 'x-ratelimit', 'cf-ray', 'server']
            for k, v in response.headers.items():
                if any(h in k.lower() for h in important_headers):
                    print(f"[OpenGraph]   {k}: {v}")
            
            # æ£€æŸ¥å“åº”å†…å®¹ï¼ˆåˆ¤æ–­æ˜¯å¦è¢«æ‹¦æˆªï¼‰
            response_preview = response.text[:1000]
            print(f"[OpenGraph] Response Preview (first 1000 chars):")
            print(f"[OpenGraph] {response_preview}")
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é”™è¯¯é¡µé¢æˆ–æ‹¦æˆªé¡µé¢
            if any(keyword in response_preview.lower() for keyword in ['access denied', 'blocked', 'captcha', '403', 'forbidden']):
                print(f"[OpenGraph] âš ï¸  è­¦å‘Šï¼šå“åº”å¯èƒ½è¢«æ‹¦æˆªæˆ–é™åˆ¶")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ OG æ ‡ç­¾
            og_title_tag = soup.find('meta', property='og:title')
            og_image_tag = soup.find('meta', property='og:image')
            og_description_tag = soup.find('meta', property='og:description')
            
            print(f"[OpenGraph] OG Tags Detection:")
            print(f"[OpenGraph]   OG Title: {'âœ… Found' if og_title_tag else 'âŒ Not Found'}")
            if og_title_tag:
                title_content = og_title_tag.get('content', '')[:100]
                print(f"[OpenGraph]     Content: {title_content}")
            print(f"[OpenGraph]   OG Image: {'âœ… Found' if og_image_tag else 'âŒ Not Found'}")
            if og_image_tag:
                image_content = og_image_tag.get('content', '')[:100]
                print(f"[OpenGraph]     Content: {image_content}")
            print(f"[OpenGraph]   OG Description: {'âœ… Found' if og_description_tag else 'âŒ Not Found'}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ JSON-LD
            jsonld_tags = soup.select('script[type="application/ld+json"]')
            print(f"[OpenGraph] JSON-LD scripts: {len(jsonld_tags)} found")
            if jsonld_tags:
                for i, tag in enumerate(jsonld_tags[:2]):  # åªæ‰“å°å‰2ä¸ª
                    try:
                        jsonld_data = json.loads(tag.string or "{}")
                        keys = list(jsonld_data.keys())[:10]
                        print(f"[OpenGraph]   JSON-LD #{i+1} keys: {keys}")
                        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æˆ–æ ‡é¢˜
                        if 'image' in jsonld_data or 'name' in jsonld_data:
                            print(f"[OpenGraph]     Contains image/name data: âœ…")
                    except Exception as e:
                        print(f"[OpenGraph]     JSON-LD #{i+1} parse error: {e}")
            
            # Pinterest ç‰¹å®šæ£€æŸ¥
            if "pinterest.com" in url.lower():
                print(f"[OpenGraph] Pinterest-specific checks:")
                pinimg_images = soup.select('img[src*="pinimg.com"], img[data-src*="pinimg.com"]')
                print(f"[OpenGraph]   pinimg.com images: {len(pinimg_images)} found")
                if pinimg_images:
                    first_img = pinimg_images[0].get('src') or pinimg_images[0].get('data-src')
                    print(f"[OpenGraph]     First image: {first_img[:80] if first_img else 'None'}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ Pinterest çš„ JavaScript æ•°æ®
                scripts_with_pinterest = [s for s in soup.select('script') if s.string and ('pinimg' in s.string.lower() or 'pinterest' in s.string.lower())]
                print(f"[OpenGraph]   Scripts with Pinterest data: {len(scripts_with_pinterest)}")
            
            print(f"[OpenGraph] ====== è¯Šæ–­ä¿¡æ¯ç»“æŸ ======")
            
            # æå– OpenGraph æ ‡ç­¾
            og_title = soup.find('meta', property='og:title')
            og_description = soup.find('meta', property='og:description')
            og_image = soup.find('meta', property='og:image')
            og_image_width = soup.find('meta', property='og:image:width')
            og_image_height = soup.find('meta', property='og:image:height')
            og_site_name = soup.find('meta', property='og:site_name')
            
            # æå–æ ‡å‡† meta æ ‡ç­¾ä½œä¸ºåå¤‡
            meta_title = soup.find('meta', attrs={'name': 'title'}) or soup.find('title')
            meta_description = soup.find('meta', attrs={'name': 'description'})
            
            result["title"] = (
                og_title.get('content', '') if og_title else
                (meta_title.string if meta_title and hasattr(meta_title, 'string') else meta_title.get('content', '')) if meta_title else
                url
            )
            
            result["description"] = (
                og_description.get('content', '') if og_description else
                meta_description.get('content', '') if meta_description else
                ''
            )
            
            # ä½¿ç”¨å¤šå±‚å–å›¾ç­–ç•¥
            image_url, image_source = get_best_image_candidate(soup, response.url)
            
            if image_url:
                # æ‰¾åˆ°äº†å›¾ç‰‡ï¼ˆé¦–å›¾æˆ– OG/Twitter Cardï¼‰ï¼Œä¸éœ€è¦æˆªå›¾
                result["image"] = image_url
                result["needs_screenshot"] = False  # æ˜ç¡®è®¾ç½®ä¸º False
                print(f"[OpenGraph] Found image via {image_source}: {image_url[:80]}...")
            else:
                # æ‰€æœ‰ HTML å±‚éƒ½æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œæ ‡è®°éœ€è¦æˆªå›¾
                result["image"] = ""
                result["needs_screenshot"] = True
                print(f"[OpenGraph] No image found in HTML, marking needs_screenshot=True")
            
            # æå–å›¾ç‰‡å°ºå¯¸ï¼ˆå¦‚æœ OpenGraph æä¾›äº†ï¼‰
            if og_image_width and og_image_width.get('content'):
                try:
                    result["image_width"] = int(og_image_width.get('content'))
                except (ValueError, TypeError):
                    result["image_width"] = None
            else:
                result["image_width"] = None
                
            if og_image_height and og_image_height.get('content'):
                try:
                    result["image_height"] = int(og_image_height.get('content'))
                except (ValueError, TypeError):
                    result["image_height"] = None
            else:
                result["image_height"] = None
            
            # å¦‚æœ OpenGraph æ²¡æœ‰æä¾›å°ºå¯¸ï¼Œå°è¯•ä»å›¾ç‰‡ URL è·å–å®é™…å°ºå¯¸
            # æ³¨æ„ï¼šå¯¹äºå°çº¢ä¹¦ã€Pinterest ç­‰éœ€è¦ç‰¹æ®Š headers çš„ç½‘ç«™ï¼Œè·³è¿‡éªŒè¯ï¼Œä¿ç•™ URL è®©å‰ç«¯æµè§ˆå™¨åŠ è½½
            if result["image"] and result["image"].startswith(('http://', 'https://')) and (not result["image_width"] or not result["image_height"]):
                # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦è·³è¿‡éªŒè¯çš„ç½‘ç«™ï¼ˆè¿™äº›ç½‘ç«™çš„ CDN å¯èƒ½å¯¹åç«¯ IP 403ï¼Œä½†æµè§ˆå™¨å¯ä»¥æ­£å¸¸åŠ è½½ï¼‰
                image_url_lower = result["image"].lower()
                url_lower = url.lower()
                
                is_xhs = ("xiaohongshu.com" in image_url_lower or 
                         "picasso-static.xiaohongshu.com" in image_url_lower or
                         "xhscdn.com" in image_url_lower or
                         "sns-webpic-qc.xhscdn.com" in image_url_lower)
                
                is_pinterest = ("pinterest.com" in url_lower or 
                               "pinimg.com" in image_url_lower or
                               "pinterest" in image_url_lower)
                
                # å¯¹äºéœ€è¦ç‰¹æ®Šå¤„ç†çš„ç½‘ç«™ï¼Œè·³è¿‡å›¾ç‰‡å°ºå¯¸éªŒè¯ï¼Œä¿ç•™ URL è®©å‰ç«¯æµè§ˆå™¨åŠ è½½
                if is_xhs:
                    print(f"[OpenGraph] Skipping image size validation for XHS (preserving URL for frontend): {result['image'][:80]}...")
                elif is_pinterest:
                    print(f"[OpenGraph] Skipping image size validation for Pinterest (preserving URL for frontend): {result['image'][:80]}...")
                else:
                    # å…¶ä»–ç½‘ç«™ï¼Œå°è¯•è·å–å›¾ç‰‡å°ºå¯¸
                    try:
                        from PIL import Image
                        from io import BytesIO
                        
                        # æ„å»º headersï¼ˆä¸º Pinterest æ·»åŠ  Refererï¼‰
                        headers = {
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            "Accept": "image/webp,image/apng,image/*,*/*;q=0.8",
                            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                            "Referer": url,  # æ·»åŠ  Refererï¼Œå¸®åŠ©æŸäº›ç½‘ç«™ï¼ˆå¦‚ Pinterestï¼‰æ­£ç¡®åŠ è½½å›¾ç‰‡
                            "Origin": url.split('/')[0] + '//' + url.split('/')[2] if '/' in url else url,
                        }
                        
                        async with httpx.AsyncClient(timeout=5.0) as img_client:
                            img_response = await img_client.get(result["image"], headers=headers)
                            if img_response.status_code == 200:
                                img_data = BytesIO(img_response.content)
                                img = Image.open(img_data)
                                w, h = img.size
                                result["image_width"] = w
                                result["image_height"] = h
                                print(f"[OpenGraph] Fetched image dimensions from URL: {w}x{h} for {url[:60]}...")
                    except Exception as e:
                        # è·å–å›¾ç‰‡å°ºå¯¸å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•æ—¥å¿—
                        # é‡è¦ï¼šä¸è¦æ¸…ç©º result["image"]ï¼Œä¿ç•™ URL è®©å‰ç«¯æµè§ˆå™¨åŠ è½½
                        print(f"[OpenGraph] Failed to fetch image dimensions from URL for {url[:60]}...: {str(e)} (preserving image URL)")
            
            result["site_name"] = og_site_name.get('content', '') if og_site_name else ''
            
            # å¦‚æœ OpenGraph æŠ“å–æˆåŠŸä¸”æœ‰å›¾ç‰‡ï¼Œç«‹å³é¢„å– embedding
            if result["image"]:
                result["success"] = True
                # ç«‹å³é¢„å– embeddingï¼ˆç­‰å¾…å®Œæˆï¼Œç¡®ä¿è¿”å›æ—¶å·²æœ‰ embeddingï¼‰
                await _prefetch_embedding(result)
                return result
            
            # å¦‚æœ OpenGraph æŠ“å–æˆåŠŸä½†æ— å›¾ç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æ¡£ç±»
            # åªæœ‰æ–‡æ¡£ç±»æ‰ä½¿ç”¨æ–‡æ¡£å¡ç‰‡ï¼Œæ™®é€šç½‘é¡µå³ä½¿æ²¡æœ‰å›¾ç‰‡ä¹Ÿè¿”å›æˆåŠŸ
            is_doc_like = _is_doc_like_url(url)
            if is_doc_like:
                print(f"[OpenGraph] No og:image found for doc-like URL, generating doc card: {url[:60]}...")
                try:
                    from doc_card_generator import generate_doc_card_data_uri, detect_doc_type
                    from urllib.parse import urlparse
                    
                    parsed = urlparse(url)
                    site_name = result.get("site_name") or parsed.netloc or ""
                    if site_name.startswith("www."):
                        site_name = site_name[4:]
                    
                    if not result.get("title") or result["title"] == url:
                        path_parts = [p for p in parsed.path.split("/") if p]
                        result["title"] = path_parts[-1] if path_parts else site_name or url
                    
                    if not result.get("site_name"):
                        result["site_name"] = site_name
                    
                    doc_card_data_uri = generate_doc_card_data_uri(
                        title=result["title"],
                        url=url,
                        site_name=result["site_name"],
                        description=result.get("description", ""),
                    )
                    
                    result["image"] = doc_card_data_uri
                    result["is_doc_card"] = True
                    result["success"] = True
                    result["doc_type"] = detect_doc_type(url, result["site_name"]).get("type", "ç½‘é¡µ")
                    # æ–‡æ¡£å¡ç‰‡ä½¿ç”¨å›ºå®šå°ºå¯¸ï¼ˆ200x150ï¼‰
                    result["image_width"] = 200
                    result["image_height"] = 150
                    # ç«‹å³é¢„å– embeddingï¼ˆç­‰å¾…å®Œæˆï¼Œç¡®ä¿è¿”å›æ—¶å·²æœ‰ embeddingï¼‰
                    await _prefetch_embedding(result)
                except Exception as card_error:
                    result["error"] = f"OpenGraph æ— å›¾ç‰‡ï¼Œå¡ç‰‡ç”Ÿæˆå¤±è´¥: {str(card_error)}"
                    result["success"] = False
            else:
                # æ™®é€šç½‘é¡µå³ä½¿æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿç®—æˆåŠŸï¼ˆè¿”å› OpenGraph æ•°æ®ï¼Œå‰ç«¯å¯ä»¥æ˜¾ç¤ºæ ‡é¢˜ç­‰ï¼‰
                result["success"] = True
                # ç«‹å³é¢„å– embeddingï¼ˆç­‰å¾…å®Œæˆï¼Œç¡®ä¿è¿”å›æ—¶å·²æœ‰ embeddingï¼‰
                await _prefetch_embedding(result)
                return result
            
    except Exception as e:
        result["error"] = str(e)
        result["success"] = False
        
        # å¦‚æœ OpenGraph æŠ“å–å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æ¡£ç±»
        # åªæœ‰æ–‡æ¡£ç±»æ‰ä½¿ç”¨æ–‡æ¡£å¡ç‰‡ï¼Œæ™®é€šç½‘é¡µè¿”å›å¤±è´¥
        is_doc_like = _is_doc_like_url(url)
        if is_doc_like:
            print(f"[OpenGraph] OpenGraph fetch failed for doc-like URL, generating doc card: {url[:60]}...")
            try:
                from doc_card_generator import generate_doc_card_data_uri, detect_doc_type
                from urllib.parse import urlparse
                
                parsed = urlparse(url)
                site_name = parsed.netloc or ""
                if site_name.startswith("www."):
                    site_name = site_name[4:]
                
                path_parts = [p for p in parsed.path.split("/") if p]
                title = path_parts[-1] if path_parts else site_name or url
                
                doc_card_data_uri = generate_doc_card_data_uri(
                    title=title,
                    url=url,
                    site_name=site_name,
                    description="ç½‘é¡µå¡ç‰‡ï¼ˆOpenGraph æŠ“å–å¤±è´¥ï¼‰",
                )
                
                result["image"] = doc_card_data_uri
                result["is_doc_card"] = True
                result["success"] = True
                result["title"] = title
                result["description"] = "ç½‘é¡µå¡ç‰‡ï¼ˆOpenGraph æŠ“å–å¤±è´¥ï¼‰"
                result["site_name"] = site_name
                result["doc_type"] = detect_doc_type(url, site_name).get("type", "ç½‘é¡µ")
                result["error"] = None  # æ¸…é™¤é”™è¯¯ï¼Œå› ä¸ºå¡ç‰‡ç”ŸæˆæˆåŠŸ
                # æ–‡æ¡£å¡ç‰‡ä½¿ç”¨å›ºå®šå°ºå¯¸ï¼ˆ200x150ï¼‰
                result["image_width"] = 200
                result["image_height"] = 150
            except Exception as card_error:
                result["error"] = f"OpenGraph æŠ“å–å¤±è´¥: {str(e)}ï¼Œå¡ç‰‡ç”Ÿæˆå¤±è´¥: {str(card_error)}"
    
    # æ³¨æ„ï¼šæ‰€æœ‰æˆåŠŸåˆ†æ”¯éƒ½å·²ç»è°ƒç”¨äº† _prefetch_embeddingï¼Œè¿™é‡Œä¸éœ€è¦å†æ¬¡è°ƒç”¨
    return result


async def _prefetch_embedding(result: Dict) -> None:
    """
    é¢„å– embedding æ•°æ®å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    ä¸€æ—¦ OpenGraph æ•°æ®è§£æå®Œæˆï¼Œç«‹å³è¯·æ±‚ embedding å¹¶å­˜å‚¨
    
    Args:
        result: OpenGraph ç»“æœå­—å…¸ï¼ˆä¼šè¢«æ›´æ–°ï¼Œæ·»åŠ  text_embedding å’Œ image_embeddingï¼‰
    """
    try:
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
        from search.embed import embed_text, embed_image
        from search.preprocess import download_image, process_image, extract_text_from_item
        from vector_db import get_opengraph_item, upsert_opengraph_item
        
        url = result.get("url", "")
        if not url:
            return
        
        # å…ˆæ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰è¯¥ URL çš„æ•°æ®ï¼ˆåŒ…æ‹¬ embeddingï¼‰
        existing_item = await get_opengraph_item(url)
        if existing_item and (existing_item.get("text_embedding") or existing_item.get("image_embedding")):
            # æ•°æ®åº“å·²æœ‰ embeddingï¼Œç›´æ¥ä½¿ç”¨
            print(f"[OpenGraph] âœ“ Found existing embeddings in DB for: {url[:60]}...")
            result["text_embedding"] = existing_item.get("text_embedding")
            result["image_embedding"] = existing_item.get("image_embedding")
            return
        
        # æ•°æ®åº“æ²¡æœ‰ï¼Œéœ€è¦ç”Ÿæˆ embedding
        title = result.get("title", "")
        description = result.get("description", "")
        image = result.get("image", "")
        is_screenshot = result.get("is_screenshot", False)
        
        # ä½¿ç”¨ pipeline çš„æ–‡æœ¬æå–é€»è¾‘
        text_content = extract_text_from_item(result)
        if not text_content:
            text_content = url  # å¦‚æœæ²¡æœ‰æ ‡é¢˜å’Œæè¿°ï¼Œä½¿ç”¨ URL
        
        # å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒ embedding
        print(f"[OpenGraph] Generating embeddings for: {url[:60]}...")
        
        # ç”Ÿæˆæ–‡æœ¬ embedding
        text_emb = None
        if text_content:
            try:
                text_emb = await embed_text(text_content)
                if text_emb:
                    result["text_embedding"] = text_emb
                    print(f"[OpenGraph] âœ“ Text embedding generated: {len(text_emb)} dims")
            except Exception as e:
                print(f"[OpenGraph] âš  Text embedding failed: {e}")
        
        # ç”Ÿæˆå›¾åƒ embedding
        image_emb = None
        if image:
            try:
                # å¤„ç†å›¾åƒï¼šå¦‚æœæ˜¯ URL éœ€è¦ä¸‹è½½ï¼Œå¦‚æœæ˜¯ Base64 ç›´æ¥ä½¿ç”¨
                if isinstance(image, str) and image.startswith("data:image"):
                    # å·²ç»æ˜¯ Base64 æ ¼å¼
                    image_emb = await embed_image(image)
                else:
                    # æ˜¯ URLï¼Œéœ€è¦ä¸‹è½½å¹¶å¤„ç†
                    image_data = await download_image(image)
                    if image_data:
                        img_b64 = process_image(image_data)
                        if img_b64:
                            image_emb = await embed_image(img_b64)
                
                if image_emb:
                    result["image_embedding"] = image_emb
                    print(f"[OpenGraph] âœ“ Image embedding generated: {len(image_emb)} dims")
            except Exception as e:
                print(f"[OpenGraph] âš  Image embedding failed: {e}")
        
        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if text_emb or image_emb:
            success = await upsert_opengraph_item(
                url=url,
                title=title,
                description=description,
                image=image,
                site_name=result.get("site_name"),
                tab_id=result.get("tab_id"),
                tab_title=result.get("tab_title"),
                text_embedding=text_emb,
                image_embedding=image_emb,
                metadata={
                    "is_doc_card": result.get("is_doc_card", False),
                    "success": result.get("success", False),
                }
            )
            if success:
                print(f"[OpenGraph] âœ“ Stored embeddings to DB for: {url[:60]}...")
            else:
                print(f"[OpenGraph] âš  Failed to store embeddings to DB for: {url[:60]}...")
        else:
            print(f"[OpenGraph] âš  No embeddings generated for: {url[:60]}...")
            
    except Exception as e:
        # é¢„å–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•æ—¥å¿—
        print(f"[OpenGraph] âš  Failed to pre-fetch embeddings for {result.get('url', '')[:60]}...: {str(e)}")
        import traceback
        traceback.print_exc()


async def fetch_multiple_opengraph(urls: List[str]) -> List[Dict]:
    """
    å¹¶å‘æŠ“å–å¤šä¸ª URL çš„ OpenGraph æ•°æ®
    """
    tasks = [fetch_opengraph(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†å¼‚å¸¸ç»“æœ
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            processed_results.append({
                "url": urls[i],
                "title": "",
                "description": "",
                "image": "",
                "site_name": "",
                "success": False,
                "error": str(result)
            })
        else:
            processed_results.append(result)
    
    return processed_results


